{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning I</font>\n",
    "\n",
    "### Mini-Projeto 5 - Reconhecimento e Classificação de Idioma em Arquivos de Áudio Usando CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook treina uma rede neural convolucional para classificar arquivos de áudio de gravações de voz nos idiomas falados. O conjunto de dados utilizado contém 65.000 arquivos em 176 idiomas e está disponível para download na página de uma competição em Data Science no site TopCoder (https://goo.gl/G5XBJl). É interessante ver que as CNNs funcionam bem em problemas onde a intuição não leva você a lugar nenhum.\n",
    "\n",
    "Obs: O dataset com os arquivos de áudio possui 4.6 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras as k\n",
    "k.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import librosa as lr\n",
    "import dask.array as da\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = (192,192,1)\n",
    "out_dim = 176\n",
    "batch_size = 32\n",
    "mp3_path = 'data/mp3/'\n",
    "tr_path = 'data/train/'\n",
    "va_path = 'data/valid/'\n",
    "te_path = 'data/test/'\n",
    "data_size = 66176\n",
    "tr_size = 52800\n",
    "va_size = 4576\n",
    "te_size = 8800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso converterá um único arquivo mp3 em um espectrograma e retornará a imagem. O mel-spectrogram é usado para obter mais informações nas frequências mais baixas, semelhantes à audição humana. As intensidades e as frequências são então escalonadas logaritmicamente. Esta função também cortará 5% do começo e fim do arquivo. Isso é para se livrar do silêncio e garantir as mesmas dimensões de cada arquivo gerado. A conversão leva cerca de 1 segundo por minuto de áudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp3_to_img(path, height=192, width=192):\n",
    "    signal, sr = lr.load(path, res_type='kaiser_fast')\n",
    "    \n",
    "    # Isso vai cortar 5% do começo e do fim\n",
    "    hl = signal.shape[0]//(width*1.1)\n",
    "    spec = lr.feature.melspectrogram(signal, n_mels=height, hop_length=int(hl))\n",
    "    img = lr.logamplitude(spec)**2\n",
    "    start = (img.shape[1] - width) // 2\n",
    "    return img[:, start:start+width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converter todos os arquivos mp3 para espectrograma jpgs. A função process_audio_with_classes() usará os rótulos para classificar todos os jpgs em subpastas correspondentes. Isso é útil para a função flow_from_directory no Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(in_folder, out_folder):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    files = glob.glob(in_folder+'*.mp3')\n",
    "    start = len(in_folder)\n",
    "    for file in files:\n",
    "        img = mp3_to_img(file)\n",
    "        sp.misc.imsave(out_folder + file[start:] + '.jpg', img)\n",
    "        \n",
    "def process_audio_with_classes(in_folder, out_folder, labels):\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    for i in range(len(labels['Sample Filename'])):\n",
    "        file = labels['Sample Filename'][i]\n",
    "        lang = labels['Language'][i]\n",
    "        os.makedirs(out_folder + lang, exist_ok=True)\n",
    "        img = mp3_to_img(in_folder+file)\n",
    "        sp.misc.imsave(out_folder + lang + '/' + file + '.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converte um diretório de imagens em um arquivo HDF5 armazenando as imagens em uma matriz. A forma da matriz será (img_num, height, width [, channels])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def jpgs_to_h5(source, target, name):\n",
    "    da.image.imread(source + '*.jpg').to_hdf5(target, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados brutos consistem em 66176 44kHz estéreo mp3 com um comprimento de 10 segundos cada. O conjunto de dados é perfeitamente balanceado com 376 arquivos por idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar esse arquivo convertendo-o em um log-mel-spectrogram.\n",
    "\n",
    "- O eixo y mostra a frequência\n",
    "- O eixo x mostra a hora\n",
    "- O cor mostra a intensidade de uma frequência em um determinado momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora seja fácil ler um espectrograma, é difícil julgar \"intuitivamente\" o conteúdo de um arquivo de áudio. Nossa primeira tentativa de convertê-los pareceu totalmente boa, mas não conseguimos treinar a rede. Usamos um espectrograma regular que depois convertemos para frequências em escala de log. O problema era que, espremendo as frequências mais altas, estávamos separando as frequências mais baixas. Essa é a ideia geral de uma escala de log, mas não levamos em conta que a resolução nas frequências mais baixas sofreria muito. \n",
    "\n",
    "Começamos com uma resolução de 224x448 pixels, mas isso levou uma eternidade. Aplicamos alguns redimensionamentos assimétricos e notamos que a suposição de reservar mais espaço para o eixo do tempo estava errada. Imagens quadradas pareciam ter melhor desempenho. Então, seguimos em frente e convertemos tudo para 192x192, o que não prejudicou muito o desempenho.\n",
    "\n",
    "A \"verificação de sanidade\" dos dados acabou por ser difícil com este conjunto de dados. Aparentemente, você pode ter 176 idiomas diferentes sem incluir inglês, alemão ou francês. Mas todas as amostras holandesas soavam como o som das pessoas holandesas, então imaginamos que não poderia ser tão errado assim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converte os arquivos mp3 para jpgs.\n",
    "process_audio('data/mp3/', 'data/jpg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converte a pasta de imagens em um arquivo contêiner compactado.\n",
    "jpgs_to_h5('data/jpg/', 'data/data.h5', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embaralhe os dados e divida-os em treino, validação e teste\n",
    "y = pd.read_csv('data/train_list.csv')['Language']\n",
    "y = pd.get_dummies(y)\n",
    "y = y.reindex_axis(sorted(y.columns), axis=1)\n",
    "y = y.values\n",
    "y = da.from_array(y, chunks=1000)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = h5py.File('data/data.h5')['data']\n",
    "x = da.from_array(x, chunks=1000)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shfl = np.random.permutation(data_size)\n",
    "\n",
    "tr_idx = shfl[:tr_size]\n",
    "va_idx = shfl[tr_size:tr_size+va_size]\n",
    "te_idx = shfl[tr_size+va_size:]\n",
    "\n",
    "x[tr_idx].to_hdf5('data/x_tr.h5', 'x_tr')\n",
    "y[tr_idx].to_hdf5('data/y_tr.h5', 'y_tr')\n",
    "x[va_idx].to_hdf5('data/x_va.h5', 'x_va')\n",
    "y[va_idx].to_hdf5('data/y_va.h5', 'y_va')\n",
    "x[te_idx].to_hdf5('data/x_te.h5', 'x_te')\n",
    "y[te_idx].to_hdf5('data/y_te.h5', 'y_te')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando e Processando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler os dados que preparamos e verificar suas dimensões\n",
    "\n",
    "x_tr = da.from_array(h5py.File('data/x_tr.h5')['x_tr'], chunks=1000)\n",
    "y_tr = da.from_array(h5py.File('data/y_tr.h5')['y_tr'], chunks=1000)\n",
    "print(x_tr.shape, y_tr.shape)\n",
    "\n",
    "x_va = da.from_array(h5py.File('data/x_va.h5')['x_va'], chunks=1000)\n",
    "y_va = da.from_array(h5py.File('data/y_va.h5')['y_va'], chunks=1000)\n",
    "print(x_va.shape, y_va.shape)\n",
    "\n",
    "x_te = da.from_array(h5py.File('data/x_te.h5')['x_te'], chunks=1000)\n",
    "y_te = da.from_array(h5py.File('data/y_te.h5')['y_te'], chunks=1000)\n",
    "print(x_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr /= 255.\n",
    "x_va /= 255.\n",
    "x_te /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos verificar uma amostra só para ter certeza\n",
    "test_img = x_tr[0, :, :, 0]\n",
    "plt.imshow(test_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentamos cerca de 30 modelos diferentes com foco em arquiteturas mais recentes, como redes residuais, redes em redes, espremendo e expandindo convoluções, mas no final um 5x-Conv-MaxPool funcionou melhor. Nós queríamos substituir as últimas camadas Dense por AveragePooling. Elas dão um pouco mais de visão do que está acontecendo em comparação com o modelo \"caixa preta\" que resulta de camadas densas. No entanto, não funcionou bem. Estamos supondo que isso é porque os espectrogramas mostram uma abstração diferente da informação em comparação com uma foto regular mostrando um objeto.\n",
    "\n",
    "Nos testes o uso de Elu substituiu a necessidade de Normalização em Lote, que normalmente melhora qualquer modelo. Não os incluímos por motivos de desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = Input(shape=in_dim)\n",
    "m = Conv2D(16, (3, 3), activation='elu', padding='same')(i)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(32, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(64, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(128, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Conv2D(256, (3, 3), activation='elu', padding='same')(m)\n",
    "m = MaxPooling2D()(m)\n",
    "m = Flatten()(m)\n",
    "m = Dense(512, activation='elu')(m)\n",
    "m = Dropout(0.5)(m)\n",
    "o = Dense(out_dim, activation='softmax')(m)\n",
    "\n",
    "model = Model(inputs=i, outputs=o)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam(lr=1e-3), metrics=['accuracy'])\n",
    "model.fit(x_tr, y_tr, epochs=2, verbose=1, validation_data=(x_va, y_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Nadam(lr=1e-4), metrics=['accuracy'])\n",
    "model.fit(x_tr, y_tr, epochs=3, verbose=1, validation_data=(x_va, y_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o modelo\n",
    "model = load_model('speech_v9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o desempenho\n",
    "model.evaluate(x_te, y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
