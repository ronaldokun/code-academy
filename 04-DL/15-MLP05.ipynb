{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning I</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo Um Algoritmo Para Rede Neural Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Custo ou Perda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, você usou pesos e bias para calcular saídas. E você usou uma função de ativação para categorizar a saída. Como você pode se lembrar, as redes neurais melhoram a precisão de suas saídas, modificando pesos e bias em resposta ao treinamento contra conjuntos de dados rotulados (aprendizagem supervisionada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existem muitas técnicas para definir a precisão de uma rede neural, que se centra na capacidade da rede de produzir valores que se aproximem dos valores corretos conhecidos. As pessoas usam nomes diferentes para essa medida de precisão, muitas vezes significando perda ou custo. Usaremos o termo custo mais frequentemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular o custo usando o erro quadrático médio (MSE). Essa é a fórmula do MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"15-MLP05-01.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url = '15-MLP05-01.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui w indica a coleta de todos os pesos na rede, b todos os bias, m é o número total de exemplos (instâncias) de treinamento, a é a aproximação de y(x) pela rede e tanto a como y(x) são vetores de mesmo comprimento.\n",
    "\n",
    "A coleção de pesos é o conjunto de todas as matrizes de peso \"achatadas\" (flattened) em vetores e concatenadas em um grande vetor. O mesmo vale para a coleta de bias, exceto que eles já são vetores, então não há necessidade de \"achatá-los\" (flatten) antes da concatenação. Veja um exemplo desta operação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matrizes 2x2\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# flatten\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma ótima maneira de abstrair todos os pesos e bias usados ​​na rede neural e torna algumas tarefas mais fáceis, como veremos daqui a pouco.\n",
    "\n",
    "O custo C (da fórmula acima), depende da diferença entre a saída correta, y(x) e a saída da rede, a. É fácil ver que nenhuma diferença entre y(x) e a (para todos os valores de x) leva a um custo de 0. Esta é a situação ideal e, de fato, o processo de aprendizagem gira em torno de minimizar o custo, tanto quanto possível.\n",
    "\n",
    "Vamos então calcular o custo!\n",
    "\n",
    "Apenas una observação: a ativação do nó sigmóide não significa nada porque a rede não possui saída rotulada para comparar. Além disso, os pesos e o bias não podem mudar e o aprendizado não pode acontecer sem um custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Neuronio(object):\n",
    "    \"\"\"\n",
    "    Classe base para os nós na rede.\n",
    "\n",
    "    Argumentos:\n",
    "\n",
    "        `nodes_entrada`: Uma lista de nós com arestas para este nó.\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes_entrada = []):\n",
    "        \"\"\"\n",
    "        O construtor do nó (é executado quando o objeto é instanciado). \n",
    "        Define propriedades que podem ser usadas por todos os nós.\n",
    "        \"\"\"\n",
    "        self.nodes_entrada = nodes_entrada\n",
    "        self.nodes_saida = []\n",
    "        self.valor = None\n",
    "        for n in nodes_entrada:\n",
    "            n.nodes_saida.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Neuronio):\n",
    "    \"\"\"\n",
    "    Uma entrada genérica na rede.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Neuronio.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Neuronio):\n",
    "    \"\"\"\n",
    "    Representa um nó que executa uma transformação linear.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # O construtor da classe base (nó). Pesos e bias são tratados como nós de entrada.\n",
    "        Neuronio.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a matemática por trás de uma transformação linear.\n",
    "        \"\"\"\n",
    "        X = self.nodes_entrada[0].valor\n",
    "        W = self.nodes_entrada[1].valor\n",
    "        b = self.nodes_entrada[2].valor\n",
    "        self.valor = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Neuronio):\n",
    "    \"\"\"\n",
    "    Representa um nó que executa a função de ativação sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Neuronio.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Este método é separado do `forward` porque ele também será usado com \"backward\".\n",
    "\n",
    "        `x`: numpy array object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Executa a função sigmoid e define o valor.\n",
    "        \"\"\"\n",
    "        input_value = self.nodes_entrada[0].valor\n",
    "        self.valor = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class CostFunction(Neuronio):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        Função do custo médio do erro. Deve ser usado como o último nó para uma rede.\n",
    "        \"\"\"\n",
    "        Neuronio.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calcula o erro quadrático médio.\n",
    "        \"\"\"\n",
    "        # NOTE: Aplicamos a função reshape() para evitar possíveis erros nas operações de matriz / vetor,\n",
    "        # conforme vimos nas aulas de álgebra linear.\n",
    "        #\n",
    "        # Tornando ambos os arrays(3,1) asseguramos que o resultado seja (3,1).\n",
    "        y = self.nodes_entrada[0].valor.reshape(-1, 1)\n",
    "        a = self.nodes_entrada[1].valor.reshape(-1, 1)\n",
    "        m = self.nodes_entrada[0].valor.shape[0]\n",
    "\n",
    "        diff = y - a\n",
    "        self.valor = np.mean(diff**2)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Classifica os nós em ordem topológica usando o Algoritmo de Kahn.\n",
    "\n",
    "    `Feed_dict`: um dicionário em que a chave é um nó ` Input` e o valor é o respectivo feed de valor para esse nó.\n",
    "\n",
    "    Retorna uma lista de nós ordenados.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.nodes_saida:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.valor = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.nodes_saida:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "   Executa uma passagem para a frente através de uma lista de nós ordenados.\n",
    "\n",
    "     Argumentos:\n",
    "\n",
    "         `Output_node`: Um nó no grafo, deve ser o nó de saída.\n",
    "         `Sorted_nodes`: uma lista topologicamente ordenada de nós.\n",
    "\n",
    "     Retorna o valor do Nó de saída\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matemática por trás do MSE reflete a equação no início deste Jupyter Notebook, onde y é saída esperada e a é gerada pela rede neural. Nós então calculamos esta diferença e elevamos o resultado ao quadrado. Por último, precisamos somar as diferenças ao quadrado e dividir pelo número total de exemplos m. Isso pode ser alcançado com np.mean ou (1 / m) * np.sum (diff ** 2).\n",
    "\n",
    "Observe que a ordem de y e a realmente não importa, nós poderíamos alterá-los (a - y) e obter o mesmo valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando o Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.499979385927558\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros de entrada\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "y = Input()\n",
    "\n",
    "# Função Linear\n",
    "f = Linear(inputs, weights, bias)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "# Função de Custo\n",
    "cost = CostFunction(y, g)\n",
    "\n",
    "# Atribuindo valores aos parâmetros\n",
    "x = np.array([[-2., -1.], [-2, -4]])\n",
    "w = np.array([[4., -6], [3., -2]])\n",
    "b = np.array([-2., -3])\n",
    "\n",
    "# Valores de saída (observados)\n",
    "array_y = np.array([[-4., -2.], [-1, -3]])\n",
    "\n",
    "# Define o feed_dict\n",
    "feed_dict = {inputs: x, weights: w, bias: b, y: array_y}\n",
    "\n",
    "# Ordena as entradas para execução\n",
    "graph = topological_sort(feed_dict)\n",
    "\n",
    "# Gera o output com o forward_pass. Perceba que usamos o objeto cost aqui!!!!!\n",
    "output = forward_pass(cost, graph)\n",
    "\n",
    "# Print\n",
    "print(cost.valor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
