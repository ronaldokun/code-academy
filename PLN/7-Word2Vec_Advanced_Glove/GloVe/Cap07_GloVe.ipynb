{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Processamento de Linguagem Natural</font>\n",
    "\n",
    "## GloVe: Global Vectors for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.7.6\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os métodos para a aprendizagem de vetores de palavras se enquadram em uma das duas categorias: métodos baseados em fatoração de matriz global ou métodos baseados em janela de contexto local. \n",
    "\n",
    "A análise semântica latente (LSA, Latent Semantic Analysis) é um exemplo de um método baseado em fatoração de matriz global, e skip-gram e CBOW são métodos baseados em janela de contexto local. O LSA é usado como uma técnica de análise de documentos que mapeia palavras nos documentos para algo conhecido como conceito, um padrão comum de palavras que aparece em um documento. \n",
    "\n",
    "Os métodos baseados na fatoração da matriz global exploram eficientemente as estatísticas globais de um corpus (por exemplo, a co-ocorrência de palavras em um escopo global), mas mostram um desempenho ruim nas tarefas de analogia de palavras. Por outro lado, os métodos baseados em janelas de contexto mostraram ter um bom desempenho em tarefas de analogia de palavras, mas não utilizam estatísticas globais do corpus, deixando espaço para melhorias. \n",
    "\n",
    "O GloVe tenta obter o melhor dos dois mundos - uma abordagem que aproveita eficientemente as estatísticas de corpus globais, enquanto otimiza o modelo de aprendizado de uma maneira baseada em janelas de contexto, semelhante a skip-gram ou CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec e GloVe aprendem codificações geométricas (vetores) de palavras a partir de suas informações de co-ocorrência (com que frequência elas aparecem juntas em corpora de texto grande). Eles diferem em que word2vec é um modelo \"preditivo\", enquanto GloVe é um modelo \"baseado em contagem\".\n",
    "\n",
    "Os modelos preditivos aprendem seus vetores a fim de melhorar sua capacidade preditiva (palavra-alvo | palavras de contexto; vetores), ou seja, a habilidade de predizer as palavras-alvo das palavras de contexto dadas às representações vetoriais. No Word2vec, isso é lançado como uma rede neural de feed-forward e otimizado como tal usando SGD, etc.\n",
    "\n",
    "Modelos baseados em contagem aprendem seus vetores essencialmente fazendo a redução de dimensionalidade na matriz de contagem de co-ocorrência. Eles primeiro constroem uma grande matriz de informações de co-ocorrência (palavras x contexto), ou seja, para cada \"palavra\" (as linhas), você conta com que frequência vemos essa palavra em algum \"contexto\" (as colunas) em um corpus grande. O número de \"contextos\" é obviamente grande, uma vez que é essencialmente combinatório em tamanho. Então, eles fatorizam essa matriz para produzir uma matriz de dimensões inferiores (palavra x features), onde cada linha agora produz uma representação vetorial para cada palavra. Em geral, isso é feito minimizando uma \"perda de reconstrução\" que tenta encontrar as representações de menor dimensão que podem explicar a maior parte da variação nos dados de alta dimensão. No caso específico do GloVe, a matriz de contagens é pré-processada normalizando as contagens e e aplicando smoothing. Isso acaba sendo positivo em termos da qualidade das representações aprendidas.\n",
    "\n",
    "No entanto, quando controlamos todos os hiperparâmetros de treinamento, as embeddings geradas usando os dois métodos tendem a ter um desempenho muito semelhante nas tarefas de PLN. Os benefícios adicionais do GloVe em relação ao Word2vec é que é mais fácil paralelizar a implementação, o que significa que é mais fácil treinar em mais dados, o que, com esses modelos, é sempre bom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 16 21:27:39 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN X (Pascal)    On   | 00000000:05:00.0 Off |                  N/A |\r\n",
      "| 23%   37C    P8     8W / 250W |    115MiB / 12194MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce GTX 108...  On   | 00000000:09:00.0 Off |                  N/A |\r\n",
      "| 23%   32C    P8     8W / 250W |      2MiB / 11178MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  TITAN RTX           On   | 00000000:0B:00.0 Off |                  N/A |\r\n",
      "| 41%   34C    P8    13W / 280W |      1MiB / 24220MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1510      G   /usr/lib/xorg/Xorg                            39MiB |\r\n",
      "|    0      1547      G   /usr/bin/gnome-shell                          72MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import bz2\n",
    "import collections\n",
    "import math\n",
    "import nltk\n",
    "import operator\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy\n",
    "from scipy.sparse import lil_matrix\n",
    "from math import ceil\n",
    "import matplotlib\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn          0.22.2\n",
      "numpy            1.18.4\n",
      "nltk             3.4.5\n",
      "scipy            1.4.1\n",
      "matplotlib.pylab 1.18.4\n",
      "matplotlib       3.2.1\n",
      "tensorflow       2.2.0\n",
      "Data Science Academy\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obs: Este script está compatível com as versões 1.x e 2.x do TensorFlow.\n",
    "# Optamos por manter assim, pois alguns recursos avançados usados neste script ainda não foram implementados no TF 2.\n",
    "\n",
    "# Para executar este script com TF 2, nenhum passo adicional precisa ser feito.\n",
    "# Para executar com TF 1, remova o prefixo tf.compat.v1 ao longo do scriipt e substitua por tf, e comente as 3 linhas abaixo.\n",
    "import tensorflow.python.util.deprecation as deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Este código faz o download de um [conjunto de dados] (http://www.evanjones.ca/software/wikipedia2text.html) que consiste em vários artigos da Wikipedia, totalizando aproximadamente 61 megabytes. Além disso, o código garante que o arquivo tenha o tamanho correto após o download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Encontrado e verificado %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Não foi possível verificar o arquivo ' + filename + '. Você consegue fazer o download via browser?')\n",
    "  return filename\n",
    "\n",
    "filename = 'dados/wikipedia2text-extracted.txt.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo dados com pré-processamento com NLTK\n",
    "Lê os dados como estão em uma string, converte em minúscula e o converte em tokens usando a biblioteca nltk. Este código lê dados em porções de 1 MB (pois o processamento do texto completo de uma só vez pode ser lento) e retorna uma lista de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo os dados...\n",
      "Tamanho do dataset 3361041\n",
      "Palavras de exemplo (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Palavras de exemplo (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "\n",
    "  with bz2.BZ2File(filename) as f:\n",
    "\n",
    "    data = []\n",
    "    file_size = os.stat(filename).st_size\n",
    "    chunk_size = 1024 * 1024\n",
    "    print('Lendo os dados...')\n",
    "    for i in range(ceil(file_size//chunk_size)+1):\n",
    "        bytes_to_read = min(chunk_size,file_size-(i*chunk_size))\n",
    "        file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "        file_string = file_string.lower()\n",
    "        file_string = nltk.word_tokenize(file_string)\n",
    "        data.extend(file_string)\n",
    "  return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Tamanho do dataset %d' % len(words))\n",
    "token_count = len(words)\n",
    "\n",
    "print('Palavras de exemplo (start): ',words[:10])\n",
    "print('Palavras de exemplo (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo os Dicionários\n",
    "Para entender cada um desses elementos, vamos também assumir o texto \"Eu gosto de ir à escola\"\n",
    "\n",
    "* `dictionary`: mapeia uma palavra para um ID (i.e. {Eu:0, gosto:1, de:2, ir:3, à:4, escola:5})\n",
    "* `reverse_dictionary`: mapeia um ID para uma palavra (i.e. {0:Eu, 1:gosto, 2:de, 3:ir, 4:à, 5:escola}\n",
    "* `count`: Lista de elementos (palavra, frequência) (i.e. [(Eu,1),(gosto,1),(de,2),(ir,1),(à,1),(escola,1)]\n",
    "* `data` : Contém a string de texto que lemos, onde palavras são substituídas por IDs de palavras (i.e. [0, 1, 2, 3, 2, 4])\n",
    "\n",
    "Também introduzimos um token especial adicional chamado `UNK` para indicar que palavras raras são muito raras para serem usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais comuns (+UNK) [['UNK', 68859], ('the', 226892), (',', 184013), ('.', 120944), ('of', 116323)]\n",
      "Dados de amostra [1721, 9, 8, 16476, 223, 4, 5166, 4457, 26, 11592]\n"
     ]
    }
   ],
   "source": [
    "# Nós restringimos o tamanho do nosso vocabulário para 50000\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "\n",
    "  # Obtém apenas o vocabulary_size para palavras mais comuns como o vocabulário\n",
    "  # Todas as outras palavras serão substituídas por token UNK\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "\n",
    "  # Cria um ID para cada palavra, dando o comprimento atual do dicionário\n",
    "  # e adicionando esse item ao dicionário\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "\n",
    "  # Percorre todo o texto que temos e produzir uma lista onde cada elemento corresponde ao ID\n",
    "  # da palavra encontrada nesse índice\n",
    "  for word in words:\n",
    "\n",
    "    # Se a palavra estiver no dicionário, use a palavra ID, senão use o ID do token especial \"UNK\"\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "\n",
    "  # Atualiza a variável count com o número de ocorrências UNK\n",
    "  count[0][1] = unk_count\n",
    "\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "  # Verifica se o dicionário é do tamanho do vocabulário\n",
    "  assert len(dictionary) == vocabulary_size\n",
    "\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Palavras mais comuns (+UNK)', count[:5])\n",
    "print('Dados de amostra', data[:10])\n",
    "del words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Lotes de Dados para o Data for GloVe\n",
    "Gera um lote ou palavras de destino (`batch`) e um lote de palavras de contexto correspondentes (`labels`). Ele lê as palavras `2 * window_size + 1` por vez (chamado` span`) e cria os datapoints `2 * window_size` em um único intervalo. A função continua dessa maneira até que os datapoints `batch_size` sejam criados. Toda vez que chegamos ao final da sequência de palavras, retornamos ao começo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "Com window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n",
      "    weights: [0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5]\n",
      "\n",
      "Com window_size = 4:\n",
      "    batch: ['set', 'set', 'set', 'set', 'set', 'set', 'set', 'set']\n",
      "    labels: ['propaganda', 'is', 'a', 'concerted', 'of', 'messages', 'aimed', 'at']\n",
      "    weights: [0.25, 0.33333334, 0.5, 1.0, 1.0, 0.5, 0.33333334, 0.25]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "  # data_index é atualizado por 1 toda vez que lemos um ponto de dados\n",
    "  global data_index\n",
    "\n",
    "  # Arrays para conter as palavras-alvo (lote), as palavras de contexto (etiquetas) e os pesos\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  weights = np.ndarray(shape=(batch_size), dtype=np.float32)\n",
    "\n",
    "  # span define o tamanho total da janela, onde os dados que consideramos em uma instância\n",
    "  # são exibidos da seguinte maneira: [skip_window target skip_window]\n",
    "  span = 2 * window_size + 1\n",
    "\n",
    "  # O buffer contém os dados contidos no intervalo\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "\n",
    "  # Preenche o buffer e atualiza o data_index\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "\n",
    "  # Este é o número de palavras de contexto que experimentamos para uma única palavra de destino\n",
    "  num_samples = 2*window_size\n",
    "\n",
    "  # Nós dividimos a leitura do lote em dois loops for\n",
    "  # O loop interno preenche o lote e os rótulos com\n",
    "  # num_samples pontos de dados usando dados contidos no intervalo\n",
    "  # O loop externo é repetido por batch_size // num_samples times\n",
    "  # para produzir um lote completo\n",
    "  for i in range(batch_size // num_samples):\n",
    "    k=0\n",
    "\n",
    "    # Evita a própria palavra alvo como uma previsão\n",
    "    # Preenchimento de matrizes de lotes e rótulos numpy\n",
    "    for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "      batch[i * num_samples + k] = buffer[window_size]\n",
    "      labels[i * num_samples + k, 0] = buffer[j]\n",
    "      weights[i * num_samples + k] = abs(1.0/(j - window_size))\n",
    "      k += 1\n",
    "\n",
    "    # Toda vez que lemos num_samples pontos de dados,\n",
    "    # criamos o número máximo de pontos de dados possíveis\n",
    "    # com um único intervalo, por isso precisamos mover o span por 1\n",
    "    # para criar um novo intervalo\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels, weights\n",
    "\n",
    "print('Dados:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for window_size in [2, 4]:\n",
    "    data_index = 0\n",
    "    batch, labels, weights = generate_batch(batch_size=8, window_size=window_size)\n",
    "    print('\\nCom window_size = %d:' %window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "    print('    weights:', [w for w in weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando a Word Co-Occurance Matrix\n",
    "Para o GloVe mostrar seu valor em relação ao método baseado em janela de contexto, ele emprega estatísticas globais do corpus no modelo. Isso é feito usando informações da matriz de co-ocorrência de palavras para otimizar os vetores de palavras. Basicamente, a entrada X (i, j) da matriz de co-ocorrência diz a frequência com que a palavra i aparece perto de j. Também usamos um mecanismo de ponderação para dar mais peso às palavras próximas umas das outras do que àquelas mais distantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50000)\n",
      "Executando 420130 iterações para computar a co-occurance matrix\n",
      "\tConcluídas 100000 iterações\n",
      "\tConcluídas 200000 iterações\n",
      "\tConcluídas 300000 iterações\n",
      "\tConcluídas 400000 iterações\n",
      "Chunks de exemplo da co-occurance matrix\n",
      "\n",
      "Palavra alvo: \"UNK\"\n",
      "Context word:\",\"(id:2,count:3243.73), \"UNK\"(id:0,count:2020.35), \"the\"(id:1,count:1982.02), \"and\"(id:5,count:1413.50), \".\"(id:3,count:1284.67), \"of\"(id:4,count:994.16), \"(\"(id:13,count:960.42), \"in\"(id:6,count:771.67), \")\"(id:12,count:762.33), \"a\"(id:8,count:549.17), \n",
      "\n",
      "Palavra alvo: \"1st\"\n",
      "Context word:\"the\"(id:1,count:11.00), \",\"(id:2,count:6.75), \".\"(id:3,count:5.08), \"century\"(id:92,count:4.00), \"edn\"(id:23000,count:3.00), \"in\"(id:6,count:2.58), \"of\"(id:4,count:2.08), \"(\"(id:13,count:1.83), \"duke\"(id:1217,count:1.50), \"bc\"(id:568,count:1.50), \n",
      "\n",
      "Palavra alvo: \"algeria\"\n",
      "Context word:\",\"(id:2,count:7.33), \".\"(id:3,count:5.17), \"in\"(id:6,count:3.83), \"of\"(id:4,count:2.83), \"for\"(id:15,count:2.33), \"UNK\"(id:0,count:2.33), \"has\"(id:35,count:2.00), \"is\"(id:9,count:2.00), \"and\"(id:5,count:1.92), \"the\"(id:1,count:1.42), \n",
      "\n",
      "Palavra alvo: \"gale\"\n",
      "Context word:\".\"(id:3,count:1.50), \"dorothy\"(id:6452,count:1.00), \"of\"(id:4,count:1.00), \"force\"(id:269,count:1.00), \"at\"(id:26,count:1.00), \"november\"(id:431,count:1.00), \"the\"(id:1,count:0.75), \",\"(id:2,count:0.67), \"among\"(id:161,count:0.50), \"1975\"(id:2235,count:0.50), \n",
      "\n",
      "Palavra alvo: \"namely\"\n",
      "Context word:\",\"(id:2,count:7.58), \"the\"(id:1,count:4.33), \"and\"(id:5,count:1.17), \"that\"(id:16,count:1.00), \"chiang\"(id:2224,count:1.00), \"spain\"(id:693,count:1.00), \"or\"(id:29,count:1.00), \"more\"(id:50,count:0.75), \"field\"(id:401,count:0.50), \"some\"(id:53,count:0.50), \n",
      "\n",
      "Palavra alvo: \"and\"\n",
      "Context word:\",\"(id:2,count:4032.45), \"the\"(id:1,count:2524.08), \"UNK\"(id:0,count:1408.67), \"of\"(id:4,count:988.08), \".\"(id:3,count:897.33), \"in\"(id:6,count:698.42), \"to\"(id:7,count:551.08), \"a\"(id:8,count:532.58), \")\"(id:12,count:424.58), \"and\"(id:5,count:323.08), \n",
      "\n",
      "Palavra alvo: \"in\"\n",
      "Context word:\"the\"(id:1,count:3739.30), \".\"(id:3,count:1863.01), \",\"(id:2,count:1845.35), \"of\"(id:4,count:754.83), \"UNK\"(id:0,count:726.58), \"and\"(id:5,count:717.83), \"a\"(id:8,count:705.83), \"to\"(id:7,count:398.58), \"in\"(id:6,count:327.75), \"was\"(id:11,count:277.00), \n",
      "\n",
      "Palavra alvo: \"to\"\n",
      "Context word:\"the\"(id:1,count:2435.33), \",\"(id:2,count:977.74), \".\"(id:3,count:684.58), \"a\"(id:8,count:597.42), \"be\"(id:30,count:572.83), \"and\"(id:5,count:563.75), \"UNK\"(id:0,count:457.00), \"of\"(id:4,count:452.75), \"in\"(id:6,count:406.92), \"to\"(id:7,count:282.58), \n",
      "\n",
      "Palavra alvo: \"a\"\n",
      "Context word:\",\"(id:2,count:1459.17), \"of\"(id:4,count:1297.00), \".\"(id:3,count:869.50), \"in\"(id:6,count:680.42), \"as\"(id:10,count:647.67), \"the\"(id:1,count:625.58), \"to\"(id:7,count:613.25), \"UNK\"(id:0,count:567.50), \"is\"(id:9,count:537.92), \"and\"(id:5,count:519.17), \n",
      "\n",
      "Palavra alvo: \"is\"\n",
      "Context word:\"the\"(id:1,count:1047.58), \",\"(id:2,count:657.58), \".\"(id:3,count:571.83), \"a\"(id:8,count:538.08), \"it\"(id:24,count:403.50), \"of\"(id:4,count:353.08), \"to\"(id:7,count:268.92), \"UNK\"(id:0,count:257.92), \"and\"(id:5,count:251.00), \"in\"(id:6,count:249.58), \n"
     ]
    }
   ],
   "source": [
    "# Estamos criando a matriz de co-ocorrência como uma matriz de coluna esparsa comprimida com scipy.\n",
    "cooc_data_index = 0\n",
    "\n",
    "# Iteramos pelo texto completo\n",
    "dataset_size = len(data)\n",
    "\n",
    "# Quantas palavras considerar à esquerda e à direita.\n",
    "skip_window = 4\n",
    "\n",
    "# A matriz esparsa que armazena a palavra co-ocorrências\n",
    "cooc_mat = lil_matrix((vocabulary_size, vocabulary_size), dtype=np.float32)\n",
    "\n",
    "print(cooc_mat.shape)\n",
    "def generate_cooc(batch_size,skip_window):\n",
    "    '''\n",
    "    Gerar matriz de co-ocorrência processando lotes de dados\n",
    "    '''\n",
    "    data_index = 0\n",
    "    print('Executando %d iterações para computar a co-occurance matrix'%(dataset_size//batch_size))\n",
    "    for i in range(dataset_size//batch_size):\n",
    "        # Imprimindo progresso\n",
    "        if i>0 and i%100000==0:\n",
    "            print('\\tConcluídas %d iterações'%i)\n",
    "\n",
    "        # Gerando um único lote de dados\n",
    "        batch, labels, weights = generate_batch(batch_size, skip_window)\n",
    "        labels = labels.reshape(-1)\n",
    "\n",
    "        # Incrementando as entradas de matriz esparsas de acordo\n",
    "        for inp,lbl,w in zip(batch,labels,weights):\n",
    "            cooc_mat[inp,lbl] += (1.0*w)\n",
    "\n",
    "# Gera a matriz\n",
    "generate_cooc(8,skip_window)\n",
    "print('Chunks de exemplo da co-occurance matrix')\n",
    "\n",
    "\n",
    "# Basicamente calcula o co-occurance mais alto de várias palavras escolhidas\n",
    "for i in range(10):\n",
    "    idx_target = i\n",
    "\n",
    "    # Obter a linha i da matriz esparsa e torná-lo denso\n",
    "    ith_row = cooc_mat.getrow(idx_target)\n",
    "    ith_row_dense = ith_row.toarray('C').reshape(-1)\n",
    "\n",
    "    # Seleciona palavras-alvo apenas com palavras razoáveis ao redor.\n",
    "    while np.sum(ith_row_dense)<10 or np.sum(ith_row_dense)>50000:\n",
    "        idx_target = np.random.randint(0,vocabulary_size)\n",
    "\n",
    "        # Obter a linha i da matriz esparsa e torná-la densa\n",
    "        ith_row = cooc_mat.getrow(idx_target)\n",
    "        ith_row_dense = ith_row.toarray('C').reshape(-1)\n",
    "\n",
    "    print('\\nPalavra alvo: \"%s\"'%reverse_dictionary[idx_target])\n",
    "\n",
    "    # Índices com maior contagem de ith_row_dense\n",
    "    sort_indices = np.argsort(ith_row_dense).reshape(-1)\n",
    "\n",
    "    # Inverter a matriz (para obter valores máximos para o início)\n",
    "    sort_indices = np.flip(sort_indices,axis=0)\n",
    "\n",
    "    # Imprimindo várias palavras de contexto para garantir que cooc_mat esteja correto\n",
    "    print('Context word:',end='')\n",
    "    for j in range(10):\n",
    "        idx_context = sort_indices[j]\n",
    "        print('\"%s\"(id:%d,count:%.2f), '%(reverse_dictionary[idx_context],idx_context,ith_row_dense[idx_context]),end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo GloVe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Definindo Hiperparâmetros\n",
    "\n",
    "Aqui nós definimos vários hiperparâmetros incluindo:\n",
    "\n",
    "- `batch_size` (quantidade de amostras em um único lote) \n",
    "- `embedding_size` (tamanho dos embedding vectors) \n",
    "- `window_size` (tamanho da janela de contexto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pontos de dados em um único lote\n",
    "batch_size = 128\n",
    "\n",
    "# Dimensão do embedding vector.\n",
    "embedding_size = 128\n",
    "\n",
    "# Quantas palavras considerar à esquerda e à direita.\n",
    "window_size = 4\n",
    "\n",
    "# Nós escolhemos um conjunto de validação aleatório para testar os vizinhos mais próximos\n",
    "# Conjunto aleatório de palavras para avaliar a similaridade.\n",
    "valid_size = 16\n",
    "\n",
    "# Nós experimentamos datapoints válidos aleatoriamente a partir de uma janela grande sem sermos sempre determinísticos\n",
    "valid_window = 50\n",
    "\n",
    "# Ao selecionar exemplos válidos, selecionamos algumas das palavras mais frequentes,\n",
    "# bem como algumas palavras moderadamente raras\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "# Número de exemplos negativos para amostra.\n",
    "num_sampled = 32\n",
    "\n",
    "# Usado para a estabilidade do log na função de perda\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Entradas e Saídas\n",
    "\n",
    "Aqui nós definimos espaços reservados (placeholders) para alimentação dos dados de entrada e saída para o treinamento (cada um de tamanho `batch_size`) e um tensor constante para conter exemplos de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Dados de entrada de treinamento (IDs de palavras de destino).\n",
    "train_dataset = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "# Dados de labels de entrada de treinamento (IDs de palavras de contexto)\n",
    "train_labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "# Dados de entrada de validação, não precisamos de um espaço reservado,\n",
    "# pois já definimos os IDs das palavras selecionadas como dados de validação\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Parâmetros do Modelo e Outras Variáveis\n",
    "Nós agora definimos diversas variáveis do TensorFlow, como uma camada embeddings (`embeddings`) e parâmetros de rede neural (`softmax_weights` e `softmax_biases`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis\n",
    "in_embeddings = tf.Variable(\n",
    "    tf.random.uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "\n",
    "in_bias_embeddings = tf.Variable(tf.random.uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')\n",
    "\n",
    "out_embeddings = tf.Variable(\n",
    "    tf.random.uniform([vocabulary_size, embedding_size], -1.0, 1.0),name='embeddings')\n",
    "\n",
    "out_bias_embeddings = tf.Variable(tf.random.uniform([vocabulary_size],0.0,0.01,dtype=tf.float32),name='embeddings_bias')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo as Computações Modelo\n",
    "\n",
    "Primeiro, definimos uma função de pesquisa para buscar os vetores embedding correspondentes para um conjunto de entradas fornecidas. Em seguida, definimos um espaço reservado que aceita os pesos de um determinado lote de pontos de dados (`weights_x`) e pesos de matriz de co-ocorrência (`x_ij`). `weights_x` mede a importância de um ponto de dados em relação ao quanto essas duas palavras co-ocorrem e `x_ij` indica o valor da matriz de co-ocorrência para a linha e coluna indicadas pelas palavras em um ponto de dados. Com estes definidos, podemos definir a perda como mostrado abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procura por embeddings para entradas e saídas\n",
    "# Temos dois espaços vetoriais de inserção separados para entradas e saídas\n",
    "embed_in = tf.nn.embedding_lookup(params=in_embeddings, ids=train_dataset)\n",
    "embed_out = tf.nn.embedding_lookup(params=out_embeddings, ids=train_labels)\n",
    "embed_bias_in = tf.nn.embedding_lookup(params=in_bias_embeddings,ids=train_dataset)\n",
    "embed_bias_out = tf.nn.embedding_lookup(params=out_bias_embeddings,ids=train_labels)\n",
    "\n",
    "# Pesos usados na função de custo\n",
    "weights_x = tf.compat.v1.placeholder(tf.float32,shape=[batch_size],name='weights_x')\n",
    "\n",
    "# Valor de co-ocorrência para cada posição\n",
    "x_ij = tf.compat.v1.placeholder(tf.float32,shape=[batch_size],name='x_ij')\n",
    "\n",
    "# Calcula a perda definida no paper do algoritmo.\n",
    "# Observe que eu não estou seguindo a equação exata dada (que está computando um par de palavras de cada vez)\n",
    "# Estou calculando a perda de um lote de uma só vez, mas os cálculos são idênticos.\n",
    "# Eu também fiz uma suposição sobre o viés, que é um tipo menor de embedding\n",
    "loss = tf.reduce_mean(\n",
    "    input_tensor=weights_x * (tf.reduce_sum(input_tensor=embed_in*embed_out,axis=1) + embed_bias_in + embed_bias_out - tf.math.log(epsilon+x_ij))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando Similaridade de palavras\n",
    "Calculamos a similaridade entre duas palavras dadas em termos da distância do cosseno. Para fazer isso de maneira eficiente, usamos operações de matriz para fazer isso, conforme mostrado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a similaridade entre os exemplos de minibatch e todos os embeddings.\n",
    "# Nós usamos a distância do cosseno:\n",
    "embeddings = (in_embeddings + out_embeddings)/2.0\n",
    "norm = tf.sqrt(tf.reduce_sum(input_tensor=tf.square(embeddings), axis=1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(\n",
    "params=normalized_embeddings, ids=valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, tf.transpose(a=normalized_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizador de Parâmetros do Modelo\n",
    "\n",
    "Em seguida, definimos uma taxa de aprendizado constante e um otimizador que usa o método Adagrad. Sinta-se à vontade para experimentar outros otimizadores listados [aqui] (https://www.tensorflow.org/api_guides/python/train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizador\n",
    "optimizer = tf.compat.v1.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o Algoritmo GloVe \n",
    "\n",
    "Aqui nós executamos o algoritmo GloVe que definimos acima. Especificamente, primeiro inicializamos as variáveis e depois treinamos o algoritmo para várias etapas (`num_steps`). E a cada poucos passos avaliamos o algoritmo em um conjunto de validação fixo e imprimimos as palavras que parecem estar mais próximas de um determinado conjunto de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variáveis Inicializadas\n",
      "Perda média no passo 0: 16.888613\n",
      "Palavra Mais Próxima de that: mountain, vaporized, 1952., pseudomonas, 1/3rd, vice-versa, left-leaning, porridge,\n",
      "Palavra Mais Próxima de not: chaktomuk, mottes, hotspot, metrics, chōng, outfitted, embarrassed, bolivian,\n",
      "Palavra Mais Próxima de its: prophesied, grudge, serie, pinpointed, onstar, 1994., patched, jealousy,\n",
      "Palavra Mais Próxima de .: are, also, the, some, and, in, splendor, contiguous,\n",
      "Palavra Mais Próxima de be: ugarte, tibetans, salle, burner, rhythm, emetic, 147, convened,\n",
      "Palavra Mais Próxima de but: sieur, yamashiro, deportations, mihail, accomplish, thulborn, fletcher, jamming,\n",
      "Palavra Mais Próxima de were: turboprop, voc, balboa, insofar, volatility, softer, drawer, green-yellow,\n",
      "Palavra Mais Próxima de (: localised, plunkett, overgrazing, fares, mikoyan, potatoes, chop, jonestown,\n",
      "Palavra Mais Próxima de ,: and, of, some, memories, courtesy, volatiles, barrel, sky,\n",
      "Palavra Mais Próxima de was: portuguese-speaking, fusions, gaff, transmigrates, fifty, successive, rear, raja,\n",
      "Palavra Mais Próxima de a: emmet, cave, aleppo, sixth, sam-e, manners, hilton, 20.7,\n",
      "Palavra Mais Próxima de ): madrigals, unosom, shaky, bequeathed, teeming, lovecraft, quasi-independent, detonating,\n",
      "Palavra Mais Próxima de 's: pencils, unknowing, khazaria, anchored, naturkunde, jealousy, andrews, single-handed,\n",
      "Palavra Mais Próxima de first: 79, stalin, 'at, execution, male, qui, near-earth, sweden,\n",
      "Palavra Mais Próxima de and: ,, of, some, ., nordisk, ist, ef-111, roman,\n",
      "Palavra Mais Próxima de it: taffy, sanskrit, browsers, fatally, hurriedly, six-week, half-million, monger,\n",
      "Perda média no passo 2000: 0.811246\n",
      "Perda média no passo 4000: 0.123013\n",
      "Perda média no passo 6000: 0.067443\n",
      "Perda média no passo 8000: 0.089514\n",
      "Perda média no passo 10000: 0.057465\n",
      "Palavra Mais Próxima de that: is, was, the, to, ,, ., and, a,\n",
      "Palavra Mais Próxima de not: it, was, is, that, be, have, they, to,\n",
      "Palavra Mais Próxima de its: for, cutters, metalanguage, prophesied, earnings, kabbalah, and, serie,\n",
      "Palavra Mais Próxima de .: the, in, of, ,, and, a, for, to,\n",
      "Palavra Mais Próxima de be: to, have, not, is, for, been, was, a,\n",
      "Palavra Mais Próxima de but: with, not, sparred, farāni, dominates, ,, was, lan,\n",
      "Palavra Mais Próxima de were: are, by, they, was, and, ,, that, .,\n",
      "Palavra Mais Próxima de (: UNK, ), '', ,, and, ``, ., in,\n",
      "Palavra Mais Próxima de ,: and, in, UNK, the, ., a, of, for,\n",
      "Palavra Mais Próxima de was: is, by, ., in, a, ,, it, to,\n",
      "Palavra Mais Próxima de a: ,, and, ., the, for, in, of, is,\n",
      "Palavra Mais Próxima de ): (, UNK, '', ,, and, in, ., the,\n",
      "Palavra Mais Próxima de 's: of, the, ., in, and, ,, UNK, from,\n",
      "Palavra Mais Próxima de first: lacks, shelter, altitude, from, joan, most, execution, 'at,\n",
      "Palavra Mais Próxima de and: ,, in, the, UNK, ., of, a, for,\n",
      "Palavra Mais Próxima de it: is, was, that, not, a, ., ,, to,\n",
      "Perda média no passo 12000: 0.049017\n",
      "Perda média no passo 14000: 0.051533\n",
      "Perda média no passo 16000: 0.052999\n",
      "Perda média no passo 18000: 0.042628\n",
      "Perda média no passo 20000: 0.038743\n",
      "Palavra Mais Próxima de that: is, was, the, ,, to, a, ., in,\n",
      "Palavra Mais Próxima de not: it, that, is, was, have, be, to, they,\n",
      "Palavra Mais Próxima de its: for, from, and, to, ,, the, in, on,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, 's, a, for,\n",
      "Palavra Mais Próxima de be: to, have, not, from, a, is, for, that,\n",
      "Palavra Mais Próxima de but: with, which, not, ,, and, sparred, was, farāni,\n",
      "Palavra Mais Próxima de were: are, by, they, was, and, ,, in, .,\n",
      "Palavra Mais Próxima de (: ), UNK, '', and, ,, ``, ., in,\n",
      "Palavra Mais Próxima de ,: and, in, the, UNK, a, ., of, with,\n",
      "Palavra Mais Próxima de was: is, by, in, ,, ., a, that, it,\n",
      "Palavra Mais Próxima de a: ,, and, the, in, ., is, for, of,\n",
      "Palavra Mais Próxima de ): (, UNK, '', ,, and, in, ., the,\n",
      "Palavra Mais Próxima de 's: the, of, ., in, and, on, ,, from,\n",
      "Palavra Mais Próxima de first: lacks, 's, from, the, of, at, shelter, in,\n",
      "Palavra Mais Próxima de and: ,, the, in, UNK, ., of, a, for,\n",
      "Palavra Mais Próxima de it: is, was, that, not, a, there, to, this,\n",
      "Perda média no passo 22000: 0.036242\n",
      "Perda média no passo 24000: 0.037369\n",
      "Perda média no passo 26000: 0.035735\n",
      "Perda média no passo 28000: 0.034460\n",
      "Perda média no passo 30000: 0.033394\n",
      "Palavra Mais Próxima de that: is, was, to, by, the, ., ,, in,\n",
      "Palavra Mais Próxima de not: it, that, is, was, were, be, to, which,\n",
      "Palavra Mais Próxima de its: for, from, and, the, in, ,, to, on,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, 's, for, is,\n",
      "Palavra Mais Próxima de be: to, have, not, from, for, a, is, that,\n",
      "Palavra Mais Próxima de but: which, with, not, ,, and, was, a, it,\n",
      "Palavra Mais Próxima de were: are, by, was, ,, and, that, is, they,\n",
      "Palavra Mais Próxima de (: ), UNK, '', and, ,, or, ``, in,\n",
      "Palavra Mais Próxima de ,: and, UNK, in, the, ., with, a, of,\n",
      "Palavra Mais Próxima de was: is, by, in, ,, ., that, a, the,\n",
      "Palavra Mais Próxima de a: ,, is, and, for, with, ., the, of,\n",
      "Palavra Mais Próxima de ): (, UNK, '', and, ,, in, ., or,\n",
      "Palavra Mais Próxima de 's: the, ., of, in, and, on, from, ,,\n",
      "Palavra Mais Próxima de first: the, 's, of, ., lacks, at, from, in,\n",
      "Palavra Mais Próxima de and: ,, UNK, the, in, ., of, with, a,\n",
      "Palavra Mais Próxima de it: is, was, that, not, has, this, a, he,\n",
      "Perda média no passo 32000: 0.033154\n",
      "Perda média no passo 34000: 0.030890\n",
      "Perda média no passo 36000: 0.029818\n",
      "Perda média no passo 38000: 0.029277\n",
      "Perda média no passo 40000: 0.029467\n",
      "Palavra Mais Próxima de that: is, was, to, it, the, by, ., ,,\n",
      "Palavra Mais Próxima de not: it, that, was, is, be, were, which, have,\n",
      "Palavra Mais Próxima de its: for, and, ,, the, to, in, from, .,\n",
      "Palavra Mais Próxima de .: the, in, of, ,, and, 's, for, is,\n",
      "Palavra Mais Próxima de be: to, have, not, from, a, for, is, that,\n",
      "Palavra Mais Próxima de but: which, with, not, ,, and, was, a, that,\n",
      "Palavra Mais Próxima de were: are, by, was, and, ,, they, that, in,\n",
      "Palavra Mais Próxima de (: ), UNK, '', or, and, ,, ``, .,\n",
      "Palavra Mais Próxima de ,: and, UNK, in, the, a, ., with, of,\n",
      "Palavra Mais Próxima de was: is, by, in, ,, a, ., it, that,\n",
      "Palavra Mais Próxima de a: ,, is, and, ., the, with, in, for,\n",
      "Palavra Mais Próxima de ): (, UNK, '', and, ,, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, of, ., in, and, on, ,, at,\n",
      "Palavra Mais Próxima de first: 's, the, of, in, ., at, from, on,\n",
      "Palavra Mais Próxima de and: ,, the, UNK, in, ., of, with, a,\n",
      "Palavra Mais Próxima de it: is, was, that, not, has, this, a, he,\n",
      "Perda média no passo 42000: 0.039870\n",
      "Perda média no passo 44000: 0.027494\n",
      "Perda média no passo 46000: 0.027276\n",
      "Perda média no passo 48000: 0.027650\n",
      "Perda média no passo 50000: 0.041402\n",
      "Palavra Mais Próxima de that: is, was, to, the, ., it, by, a,\n",
      "Palavra Mais Próxima de not: it, that, is, was, which, were, be, are,\n",
      "Palavra Mais Próxima de its: for, and, the, ,, in, to, from, on,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, a, for, 's,\n",
      "Palavra Mais Próxima de be: to, have, not, from, a, is, for, that,\n",
      "Palavra Mais Próxima de but: which, not, with, ,, and, was, a, it,\n",
      "Palavra Mais Próxima de were: are, by, was, and, ,, in, they, to,\n",
      "Palavra Mais Próxima de (: ), '', or, UNK, and, ,, ``, .,\n",
      "Palavra Mais Próxima de ,: and, in, the, a, ., with, of, for,\n",
      "Palavra Mais Próxima de was: is, by, in, a, ,, ., it, that,\n",
      "Palavra Mais Próxima de a: ,, is, ., the, of, with, and, in,\n",
      "Palavra Mais Próxima de ): (, '', and, ,, UNK, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, of, ., in, on, and, ,, at,\n",
      "Palavra Mais Próxima de first: the, of, 's, at, ., in, from, world,\n",
      "Palavra Mais Próxima de and: ,, the, in, of, ., with, a, for,\n",
      "Palavra Mais Próxima de it: is, was, that, has, not, this, a, he,\n",
      "Perda média no passo 52000: 0.093201\n",
      "Perda média no passo 54000: 0.030739\n",
      "Perda média no passo 56000: 0.061987\n",
      "Perda média no passo 58000: 0.026611\n",
      "Perda média no passo 60000: 0.024107\n",
      "Palavra Mais Próxima de that: is, was, to, ., the, ,, it, by,\n",
      "Palavra Mais Próxima de not: it, that, is, was, were, be, which, but,\n",
      "Palavra Mais Próxima de its: for, and, the, ,, in, to, on, from,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, for, a, 's,\n",
      "Palavra Mais Próxima de be: to, have, not, from, a, is, for, an,\n",
      "Palavra Mais Próxima de but: which, not, with, ,, was, and, it, a,\n",
      "Palavra Mais Próxima de were: are, by, was, and, ,, to, in, that,\n",
      "Palavra Mais Próxima de (: ), '', UNK, or, and, ,, ., ``,\n",
      "Palavra Mais Próxima de ,: and, in, the, ., a, with, of, UNK,\n",
      "Palavra Mais Próxima de was: is, by, in, a, ,, ., it, that,\n",
      "Palavra Mais Próxima de a: is, ,, ., the, for, and, in, of,\n",
      "Palavra Mais Próxima de ): (, '', UNK, ,, and, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, of, ., in, on, and, ,, at,\n",
      "Palavra Mais Próxima de first: the, 's, of, in, ., at, on, from,\n",
      "Palavra Mais Próxima de and: ,, the, in, ., of, with, UNK, a,\n",
      "Palavra Mais Próxima de it: is, was, has, that, not, this, he, a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perda média no passo 62000: 0.023637\n",
      "Perda média no passo 64000: 0.023344\n",
      "Perda média no passo 66000: 0.023063\n",
      "Perda média no passo 68000: 0.022504\n",
      "Perda média no passo 70000: 0.022838\n",
      "Palavra Mais Próxima de that: is, was, to, the, ., by, it, ,,\n",
      "Palavra Mais Próxima de not: it, that, is, was, were, but, be, which,\n",
      "Palavra Mais Próxima de its: for, and, the, ,, in, to, on, from,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, 's, a, for,\n",
      "Palavra Mais Próxima de be: to, have, not, can, a, from, for, is,\n",
      "Palavra Mais Próxima de but: which, not, with, ,, and, was, it, a,\n",
      "Palavra Mais Próxima de were: are, was, by, and, ,, in, they, to,\n",
      "Palavra Mais Próxima de (: ), UNK, '', or, ,, and, ``, .,\n",
      "Palavra Mais Próxima de ,: and, in, the, UNK, ., a, with, of,\n",
      "Palavra Mais Próxima de was: is, by, a, in, ., it, ,, that,\n",
      "Palavra Mais Próxima de a: ,, is, and, ., with, the, for, in,\n",
      "Palavra Mais Próxima de ): (, UNK, '', ,, and, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, ., of, in, on, and, ,, from,\n",
      "Palavra Mais Próxima de first: 's, the, at, of, in, ., on, world,\n",
      "Palavra Mais Próxima de and: ,, the, in, UNK, ., of, a, with,\n",
      "Palavra Mais Próxima de it: is, was, has, that, not, this, a, he,\n",
      "Perda média no passo 72000: 0.022160\n",
      "Perda média no passo 74000: 0.021470\n",
      "Perda média no passo 76000: 0.021159\n",
      "Perda média no passo 78000: 0.020971\n",
      "Perda média no passo 80000: 0.025648\n",
      "Palavra Mais Próxima de that: is, was, to, the, ., a, ,, it,\n",
      "Palavra Mais Próxima de not: it, that, is, but, was, be, were, which,\n",
      "Palavra Mais Próxima de its: for, and, the, ,, to, in, from, .,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, a, for, 's,\n",
      "Palavra Mais Próxima de be: to, have, not, can, a, from, that, is,\n",
      "Palavra Mais Próxima de but: which, not, with, ,, was, and, it, that,\n",
      "Palavra Mais Próxima de were: are, by, and, ,, was, in, to, that,\n",
      "Palavra Mais Próxima de (: ), UNK, '', or, and, ,, ., ``,\n",
      "Palavra Mais Próxima de ,: and, UNK, the, in, ., with, a, of,\n",
      "Palavra Mais Próxima de was: is, by, in, that, a, ., ,, it,\n",
      "Palavra Mais Próxima de a: ,, is, ., and, the, for, with, of,\n",
      "Palavra Mais Próxima de ): (, UNK, '', and, ,, or, ., in,\n",
      "Palavra Mais Próxima de 's: the, ., of, in, and, on, ,, from,\n",
      "Palavra Mais Próxima de first: 's, the, at, of, ., in, on, world,\n",
      "Palavra Mais Próxima de and: ,, the, UNK, in, ., of, with, a,\n",
      "Palavra Mais Próxima de it: is, was, has, that, not, this, a, he,\n",
      "Perda média no passo 82000: 0.020696\n",
      "Perda média no passo 84000: 0.020909\n",
      "Perda média no passo 86000: 0.019555\n",
      "Perda média no passo 88000: 0.019618\n",
      "Perda média no passo 90000: 0.019458\n",
      "Palavra Mais Próxima de that: is, was, to, the, ., ,, by, it,\n",
      "Palavra Mais Próxima de not: it, that, but, is, be, was, can, which,\n",
      "Palavra Mais Próxima de its: for, and, the, ,, in, to, from, their,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, a, 's, for,\n",
      "Palavra Mais Próxima de be: to, have, can, not, that, from, a, is,\n",
      "Palavra Mais Próxima de but: which, not, ,, with, was, and, it, that,\n",
      "Palavra Mais Próxima de were: are, and, by, ,, in, the, was, to,\n",
      "Palavra Mais Próxima de (: ), UNK, '', or, and, ,, ., ``,\n",
      "Palavra Mais Próxima de ,: and, UNK, in, the, a, ., with, of,\n",
      "Palavra Mais Próxima de was: is, by, a, in, ., ,, that, it,\n",
      "Palavra Mais Próxima de a: ,, is, ., with, of, and, the, for,\n",
      "Palavra Mais Próxima de ): (, UNK, '', ,, and, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, of, ., in, and, ,, on, from,\n",
      "Palavra Mais Próxima de first: 's, the, of, at, ., in, from, on,\n",
      "Palavra Mais Próxima de and: ,, UNK, the, in, ., of, with, a,\n",
      "Palavra Mais Próxima de it: is, was, that, has, this, not, a, he,\n",
      "Perda média no passo 92000: 0.019507\n",
      "Perda média no passo 94000: 0.019612\n",
      "Perda média no passo 96000: 0.018790\n",
      "Perda média no passo 98000: 0.018526\n",
      "Perda média no passo 100000: 0.018390\n",
      "Palavra Mais Próxima de that: is, was, to, the, ., ,, a, it,\n",
      "Palavra Mais Próxima de not: it, that, but, be, is, can, which, was,\n",
      "Palavra Mais Próxima de its: for, and, the, their, ,, to, in, from,\n",
      "Palavra Mais Próxima de .: the, in, of, and, ,, for, is, a,\n",
      "Palavra Mais Próxima de be: to, have, can, not, a, that, from, would,\n",
      "Palavra Mais Próxima de but: which, not, ,, with, was, and, it, that,\n",
      "Palavra Mais Próxima de were: are, and, by, was, ,, in, to, that,\n",
      "Palavra Mais Próxima de (: ), UNK, '', or, and, ,, ``, .,\n",
      "Palavra Mais Próxima de ,: and, UNK, the, in, with, ., a, of,\n",
      "Palavra Mais Próxima de was: is, by, in, ., a, ,, that, the,\n",
      "Palavra Mais Próxima de a: is, ,, with, for, ., and, the, of,\n",
      "Palavra Mais Próxima de ): (, UNK, '', and, ,, or, in, .,\n",
      "Palavra Mais Próxima de 's: the, ., of, in, on, and, ,, UNK,\n",
      "Palavra Mais Próxima de first: 's, the, ., at, of, in, on, by,\n",
      "Palavra Mais Próxima de and: ,, UNK, the, in, ., of, with, a,\n",
      "Palavra Mais Próxima de it: is, was, that, has, not, this, he, also,\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "glove_loss = []\n",
    "\n",
    "average_loss = 0\n",
    "with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True)) as session:\n",
    "\n",
    "    tf.compat.v1.global_variables_initializer().run()\n",
    "    print('Variáveis Inicializadas')\n",
    "\n",
    "    for step in range(num_steps):\n",
    "\n",
    "        # Gera um único lote de dados (data,labels,co-occurance weights)\n",
    "        batch_data, batch_labels, batch_weights = generate_batch(batch_size, skip_window)\n",
    "\n",
    "        # Calculando os pesos exigidos pela função de perda\n",
    "        # Ponderação usada na função de perda\n",
    "        batch_weights = []\n",
    "\n",
    "        # Frequência ponderada de encontrar perto de j\n",
    "        batch_xij = []\n",
    "\n",
    "        # Calcular os pesos para cada ponto de dados no lote\n",
    "        for inp,lbl in zip(batch_data,batch_labels.reshape(-1)):\n",
    "            point_weight = (cooc_mat[inp,lbl]/100.0)**0.75 if cooc_mat[inp,lbl]<100.0 else 1.0\n",
    "            batch_weights.append(point_weight)\n",
    "            batch_xij.append(cooc_mat[inp,lbl])\n",
    "        batch_weights = np.clip(batch_weights,-100,1)\n",
    "        batch_xij = np.asarray(batch_xij)\n",
    "\n",
    "        # Preencha o feed_dict e execute o otimizador (minimize a perda) e calcule a perda.\n",
    "        # Especificamente nós fornecemos train_dataset / train_labels: treinamento de entradas e rótulos de treinamento\n",
    "        # weights_x: mede a importância de um ponto de dados em relação ao quanto essas duas palavras co-ocorrem\n",
    "        # x_ij: valor da matriz de co-ocorrência para a linha e coluna indicadas pelas palavras em um ponto de dados\n",
    "        feed_dict = {train_dataset : batch_data.reshape(-1), train_labels : batch_labels.reshape(-1),\n",
    "                    weights_x:batch_weights,x_ij:batch_xij}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        # Atualiza a variável de perda média\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "          if step > 0:\n",
    "            average_loss = average_loss / 2000\n",
    "\n",
    "          # A perda média é uma estimativa da perda nos últimos 2000 lotes.\n",
    "          print('Perda média no passo %d: %f' % (step, average_loss))\n",
    "          glove_loss.append(average_loss)\n",
    "          average_loss = 0\n",
    "\n",
    "        # Aqui calculamos as palavras top_k mais próximas para uma determinada palavra de validação\n",
    "        # em termos da distância do coseno\n",
    "        # Fazemos isso para todas as palavras no conjunto de validação\n",
    "        if step % 10000 == 0:\n",
    "          sim = similarity.eval()\n",
    "          for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8 # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "            log = 'Palavra Mais Próxima de %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "              close_word = reverse_dictionary[nearest[k]]\n",
    "              log = '%s %s,' % (log, close_word)\n",
    "            print(log)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
