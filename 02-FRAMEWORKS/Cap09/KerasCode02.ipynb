{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning Frameworks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historicamente, Perceptron foi o nome dado a um modelo de rede neural com uma única camada linear. Se o modelo tem múltiplas camadas, chamamos de Perceptron Multicamada (MLP). Cada nó na primeira camada recebe uma entrada e dispara de acordo com os limites de decisão locais predefinidos (threesholds). Em seguida, a saída da primeira camada é passada para a segunda camada, cujos resultados são passados para a camada de saída final consistindo de um único neurônio. \n",
    "\n",
    "A rede pode ser densa, o que significa que cada neurônio em uma camada está conectado a todos os neurônios localizados na camada anterior e a todos os neurônios na camada seguinte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar um único neurônio. Quais são as melhores escolhas para o peso w e o bias b? Idealmente, gostaríamos de fornecer um conjunto de exemplos de treinamento e deixar o computador ajustar o peso e o bias de tal forma que os erros produzidos na saída sejam minimizados. A fim de tornar isso um pouco mais concreto, vamos supor que temos um conjunto de imagens de gatos e outro conjunto separado de imagens que não contenham gatos. Por uma questão de simplicidade, suponha que cada neurônio olhe para um único valor de pixel de entrada. Enquanto o computador processa essas imagens, gostaríamos que nosso neurônio ajustasse seus pesos e bias para que tenhamos menos e menos imagens erroneamente reconhecidas como não-gatos. Essa abordagem parece muito intuitiva, mas exige que uma pequena alteração nos pesos (e/ou bias) cause apenas uma pequena mudança nas saídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tivermos um grande salto de saída, não podemos aprender progressivamente. Afinal, as crianças aprendem pouco a pouco. Infelizmente, o perceptron não mostra esse comportamento \"pouco a pouco\". Um perceptron é 0 ou 1 e isso é um grande salto e não vai ajudá-lo a aprender. Precisamos de algo diferente, mais suave. Precisamos de uma função que mude progressivamente de 0 para 1 sem descontinuidade. Matematicamente, isso significa que precisamos de uma função contínua que nos permita calcular a derivada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada neurônio pode ser inicializado com pesos específicos. Keras oferece algumas opções, a mais comum das quais são listadas da seguinte forma:\n",
    "\n",
    "Random_uniform: Os pesos são inicializados com valores uniformemente pequenos e aleatórios em (-0,05, 0,05). \n",
    "\n",
    "Random_normal: Os pesos são inicializados de acordo com uma distribuição Gaussiana, com média zero e pequeno desvio padrão de 0,05. \n",
    "\n",
    "Zero: Todos os pesos são inicializados para zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializadores em Keras: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de Ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Sigmóide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmóide é uma função matemática de amplo uso em campos como a economia e a computação. O nome \"sigmóide\" vem da forma em S do seu gráfico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um neurônio pode usar o sigmóide para calcular a função não-linear. Um neurônio com ativação sigmóide tem um comportamento semelhante ao perceptron, mas as mudanças são graduais e os valores de saída, como 0.3537 ou 0.147191, são perfeitamente legítimos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativação sigmóide é comumente utilizada por redes neurais com propagação positiva (Feedforward) que precisam ter como saída apenas números positivos, em redes neurais multicamadas e em outras redes com sinais contínuos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sigmóide não é o único tipo de função de ativação suave usada para redes neurais. Recentemente, uma função muito simples chamada unidade linear rectificada (ReLU) tornou-se muito popular porque gera resultados experimentais muito bons. Uma ReLU é simplesmente definida como uma função não-linear e a função é zero para valores negativos e cresce linearmente para valores positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid e ReLU são geralmente chamados funções de ativação das redes neurais. Essas mudanças graduais, típicas das funções Sigmóide e ReLU, são os blocos básicos para o desenvolvimento de um algoritmo de aprendizado que se adapta pouco a pouco, reduzindo progressivamente os erros cometidos pelas redes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconhecimento de Dígitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir uma rede que pode reconhecer números manuscritos. Para alcançar esse objetivo, usamos o MNIST (http://yann.lecun.com/exdb/mnist), um banco de dados de dígitos manuscritos composto por um conjunto de treinamento de 60.000 exemplos e um conjunto de testes de 10.000 exemplares. Os exemplos de treinamento são anotados por humanos com a resposta correta. Por exemplo, se o dígito manuscrito for o número três, então três é simplesmente o rótulo associado a esse exemplo. Cada imagem MNIST está na escala de cinza (gray scale), e consiste em 28 x 28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em muitas aplicações, é conveniente transformar features categóricas (não-numéricas) em variáveis numéricas. Por exemplo, o dígito de feature categórica com o valor d em [0-9] pode ser codificado em um vetor binário com 10 posições, que sempre tem valor 0, exceto a d-ésima posição onde um 1 está presente. Este tipo de representação é chamado de One-Hot Encoding (OHE) e é muito comum na mineração de dados quando o algoritmo de aprendizagem é especializado para lidar com funções numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rede neural com apenas uma camada oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print (tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9172\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3111 - accuracy: 0.9135 - val_loss: 0.2985 - val_accuracy: 0.9172\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3105 - accuracy: 0.9136 - val_loss: 0.2980 - val_accuracy: 0.9174\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3099 - accuracy: 0.9138 - val_loss: 0.2976 - val_accuracy: 0.9178\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3093 - accuracy: 0.9144 - val_loss: 0.2971 - val_accuracy: 0.9179\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3087 - accuracy: 0.9143 - val_loss: 0.2967 - val_accuracy: 0.9177\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3082 - accuracy: 0.9144 - val_loss: 0.2963 - val_accuracy: 0.9178\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3076 - accuracy: 0.9147 - val_loss: 0.2960 - val_accuracy: 0.9179\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3071 - accuracy: 0.9148 - val_loss: 0.2955 - val_accuracy: 0.9173\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3066 - accuracy: 0.9150 - val_loss: 0.2952 - val_accuracy: 0.9172\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3061 - accuracy: 0.9149 - val_loss: 0.2948 - val_accuracy: 0.9178\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3056 - accuracy: 0.9151 - val_loss: 0.2944 - val_accuracy: 0.9180\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3051 - accuracy: 0.9156 - val_loss: 0.2941 - val_accuracy: 0.9179\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3047 - accuracy: 0.9155 - val_loss: 0.2937 - val_accuracy: 0.9178\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3042 - accuracy: 0.9158 - val_loss: 0.2933 - val_accuracy: 0.9181\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3038 - accuracy: 0.9158 - val_loss: 0.2931 - val_accuracy: 0.9181\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3033 - accuracy: 0.9160 - val_loss: 0.2928 - val_accuracy: 0.9183\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3029 - accuracy: 0.9161 - val_loss: 0.2924 - val_accuracy: 0.9183\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3024 - accuracy: 0.9160 - val_loss: 0.2921 - val_accuracy: 0.9185\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.9161 - val_loss: 0.2918 - val_accuracy: 0.9185\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3016 - accuracy: 0.9162 - val_loss: 0.2915 - val_accuracy: 0.9187\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3012 - accuracy: 0.9164 - val_loss: 0.2912 - val_accuracy: 0.9187\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3008 - accuracy: 0.9166 - val_loss: 0.2909 - val_accuracy: 0.9187\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3004 - accuracy: 0.9166 - val_loss: 0.2906 - val_accuracy: 0.9187\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3000 - accuracy: 0.9167 - val_loss: 0.2903 - val_accuracy: 0.9190\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2996 - accuracy: 0.9166 - val_loss: 0.2901 - val_accuracy: 0.9189\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2992 - accuracy: 0.9170 - val_loss: 0.2898 - val_accuracy: 0.9188\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2988 - accuracy: 0.9171 - val_loss: 0.2895 - val_accuracy: 0.9191\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2985 - accuracy: 0.9169 - val_loss: 0.2893 - val_accuracy: 0.9189\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2981 - accuracy: 0.9174 - val_loss: 0.2889 - val_accuracy: 0.9190\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2978 - accuracy: 0.9172 - val_loss: 0.2887 - val_accuracy: 0.9193\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2974 - accuracy: 0.9174 - val_loss: 0.2884 - val_accuracy: 0.9191\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.9176 - val_loss: 0.2883 - val_accuracy: 0.9193\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2967 - accuracy: 0.9174 - val_loss: 0.2881 - val_accuracy: 0.9193\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2964 - accuracy: 0.9177 - val_loss: 0.2878 - val_accuracy: 0.9192\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2961 - accuracy: 0.9179 - val_loss: 0.2876 - val_accuracy: 0.9194\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2957 - accuracy: 0.9179 - val_loss: 0.2874 - val_accuracy: 0.9195\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2954 - accuracy: 0.9179 - val_loss: 0.2871 - val_accuracy: 0.9195\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2951 - accuracy: 0.9180 - val_loss: 0.2869 - val_accuracy: 0.9193\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2948 - accuracy: 0.9181 - val_loss: 0.2867 - val_accuracy: 0.9193\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2945 - accuracy: 0.9182 - val_loss: 0.2864 - val_accuracy: 0.9197\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2942 - accuracy: 0.9181 - val_loss: 0.2863 - val_accuracy: 0.9196\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2939 - accuracy: 0.9184 - val_loss: 0.2861 - val_accuracy: 0.9194\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2936 - accuracy: 0.9183 - val_loss: 0.2858 - val_accuracy: 0.9194\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2933 - accuracy: 0.9184 - val_loss: 0.2857 - val_accuracy: 0.9199\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2930 - accuracy: 0.9188 - val_loss: 0.2854 - val_accuracy: 0.9199\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2927 - accuracy: 0.9186 - val_loss: 0.2853 - val_accuracy: 0.9201\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2925 - accuracy: 0.9188 - val_loss: 0.2851 - val_accuracy: 0.9202\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2922 - accuracy: 0.9188 - val_loss: 0.2849 - val_accuracy: 0.9202\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2919 - accuracy: 0.9190 - val_loss: 0.2847 - val_accuracy: 0.9201\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2916 - accuracy: 0.9193 - val_loss: 0.2845 - val_accuracy: 0.9209\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2914 - accuracy: 0.9193 - val_loss: 0.2843 - val_accuracy: 0.9200\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2911 - accuracy: 0.9192 - val_loss: 0.2841 - val_accuracy: 0.9202\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2909 - accuracy: 0.9190 - val_loss: 0.2839 - val_accuracy: 0.9203\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2906 - accuracy: 0.9191 - val_loss: 0.2838 - val_accuracy: 0.9202\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2904 - accuracy: 0.9193 - val_loss: 0.2836 - val_accuracy: 0.9208\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2901 - accuracy: 0.9192 - val_loss: 0.2835 - val_accuracy: 0.9202\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2899 - accuracy: 0.9196 - val_loss: 0.2834 - val_accuracy: 0.9206\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2896 - accuracy: 0.9195 - val_loss: 0.2831 - val_accuracy: 0.9209\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.9194 - val_loss: 0.2830 - val_accuracy: 0.9201\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2892 - accuracy: 0.9197 - val_loss: 0.2828 - val_accuracy: 0.9203\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.9195 - val_loss: 0.2827 - val_accuracy: 0.9205\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2887 - accuracy: 0.9199 - val_loss: 0.2826 - val_accuracy: 0.9207\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2884 - accuracy: 0.9201 - val_loss: 0.2825 - val_accuracy: 0.9208\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2882 - accuracy: 0.9197 - val_loss: 0.2822 - val_accuracy: 0.9202\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2880 - accuracy: 0.9198 - val_loss: 0.2821 - val_accuracy: 0.9202\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2878 - accuracy: 0.9201 - val_loss: 0.2819 - val_accuracy: 0.9207\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2876 - accuracy: 0.9200 - val_loss: 0.2818 - val_accuracy: 0.9207\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2874 - accuracy: 0.9201 - val_loss: 0.2816 - val_accuracy: 0.9206\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2871 - accuracy: 0.9204 - val_loss: 0.2814 - val_accuracy: 0.9206\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2869 - accuracy: 0.9201 - val_loss: 0.2813 - val_accuracy: 0.9203\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2867 - accuracy: 0.9203 - val_loss: 0.2812 - val_accuracy: 0.9209\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2865 - accuracy: 0.9207 - val_loss: 0.2810 - val_accuracy: 0.9208\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2863 - accuracy: 0.9204 - val_loss: 0.2809 - val_accuracy: 0.9207\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2861 - accuracy: 0.9205 - val_loss: 0.2809 - val_accuracy: 0.9211\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2859 - accuracy: 0.9205 - val_loss: 0.2807 - val_accuracy: 0.9209\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2857 - accuracy: 0.9208 - val_loss: 0.2806 - val_accuracy: 0.9208\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2855 - accuracy: 0.9207 - val_loss: 0.2805 - val_accuracy: 0.9212\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2853 - accuracy: 0.9208 - val_loss: 0.2803 - val_accuracy: 0.9208\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.9208 - val_loss: 0.2802 - val_accuracy: 0.9214\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2849 - accuracy: 0.9211 - val_loss: 0.2801 - val_accuracy: 0.9208\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2847 - accuracy: 0.9209 - val_loss: 0.2800 - val_accuracy: 0.9214\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2845 - accuracy: 0.9210 - val_loss: 0.2799 - val_accuracy: 0.9215\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2844 - accuracy: 0.9213 - val_loss: 0.2798 - val_accuracy: 0.9212\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2841 - accuracy: 0.9211 - val_loss: 0.2796 - val_accuracy: 0.9213\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2840 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9218\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2838 - accuracy: 0.9211 - val_loss: 0.2794 - val_accuracy: 0.9213\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2836 - accuracy: 0.9213 - val_loss: 0.2793 - val_accuracy: 0.9216\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2834 - accuracy: 0.9210 - val_loss: 0.2791 - val_accuracy: 0.9218\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2832 - accuracy: 0.9210 - val_loss: 0.2792 - val_accuracy: 0.9218\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2831 - accuracy: 0.9212 - val_loss: 0.2790 - val_accuracy: 0.9218\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2829 - accuracy: 0.9215 - val_loss: 0.2789 - val_accuracy: 0.9221\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2827 - accuracy: 0.9215 - val_loss: 0.2788 - val_accuracy: 0.9218\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2826 - accuracy: 0.9214 - val_loss: 0.2787 - val_accuracy: 0.9220\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2824 - accuracy: 0.9215 - val_loss: 0.2786 - val_accuracy: 0.9223\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2822 - accuracy: 0.9218 - val_loss: 0.2784 - val_accuracy: 0.9220\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2821 - accuracy: 0.9216 - val_loss: 0.2783 - val_accuracy: 0.9218\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2819 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9218\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2817 - accuracy: 0.9215 - val_loss: 0.2781 - val_accuracy: 0.9220\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2816 - accuracy: 0.9214 - val_loss: 0.2781 - val_accuracy: 0.9224\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2814 - accuracy: 0.9218 - val_loss: 0.2779 - val_accuracy: 0.9222\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2812 - accuracy: 0.9216 - val_loss: 0.2779 - val_accuracy: 0.9225\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2811 - accuracy: 0.9216 - val_loss: 0.2778 - val_accuracy: 0.9227\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2809 - accuracy: 0.9216 - val_loss: 0.2777 - val_accuracy: 0.9225\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2808 - accuracy: 0.9219 - val_loss: 0.2776 - val_accuracy: 0.9222\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2806 - accuracy: 0.9219 - val_loss: 0.2774 - val_accuracy: 0.9221\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2805 - accuracy: 0.9220 - val_loss: 0.2774 - val_accuracy: 0.9224\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2803 - accuracy: 0.9220 - val_loss: 0.2773 - val_accuracy: 0.9227\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2802 - accuracy: 0.9222 - val_loss: 0.2772 - val_accuracy: 0.9222\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2800 - accuracy: 0.9220 - val_loss: 0.2771 - val_accuracy: 0.9227\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2799 - accuracy: 0.9220 - val_loss: 0.2769 - val_accuracy: 0.9226\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2797 - accuracy: 0.9222 - val_loss: 0.2769 - val_accuracy: 0.9228\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2796 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9225\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2794 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9229\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2793 - accuracy: 0.9222 - val_loss: 0.2767 - val_accuracy: 0.9224\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2792 - accuracy: 0.9220 - val_loss: 0.2766 - val_accuracy: 0.9227\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2790 - accuracy: 0.9223 - val_loss: 0.2765 - val_accuracy: 0.9230\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2788 - accuracy: 0.9224 - val_loss: 0.2764 - val_accuracy: 0.9233\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2788 - accuracy: 0.9223 - val_loss: 0.2763 - val_accuracy: 0.9233\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2786 - accuracy: 0.9221 - val_loss: 0.2762 - val_accuracy: 0.9229\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2784 - accuracy: 0.9224 - val_loss: 0.2762 - val_accuracy: 0.9230\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2783 - accuracy: 0.9221 - val_loss: 0.2760 - val_accuracy: 0.9232\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2782 - accuracy: 0.9225 - val_loss: 0.2761 - val_accuracy: 0.9231\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2781 - accuracy: 0.9225 - val_loss: 0.2759 - val_accuracy: 0.9231\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2779 - accuracy: 0.9225 - val_loss: 0.2758 - val_accuracy: 0.9232\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2778 - accuracy: 0.9224 - val_loss: 0.2758 - val_accuracy: 0.9231\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.9222 - val_loss: 0.2757 - val_accuracy: 0.9231\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2775 - accuracy: 0.9226 - val_loss: 0.2757 - val_accuracy: 0.9233\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2774 - accuracy: 0.9225 - val_loss: 0.2755 - val_accuracy: 0.9233\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2773 - accuracy: 0.9225 - val_loss: 0.2754 - val_accuracy: 0.9231\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2772 - accuracy: 0.9225 - val_loss: 0.2754 - val_accuracy: 0.9233\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2770 - accuracy: 0.9229 - val_loss: 0.2754 - val_accuracy: 0.9233\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2769 - accuracy: 0.9226 - val_loss: 0.2752 - val_accuracy: 0.9233\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2768 - accuracy: 0.9227 - val_loss: 0.2752 - val_accuracy: 0.9237\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2767 - accuracy: 0.9229 - val_loss: 0.2751 - val_accuracy: 0.9231\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2765 - accuracy: 0.9229 - val_loss: 0.2752 - val_accuracy: 0.9239\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2764 - accuracy: 0.9229 - val_loss: 0.2750 - val_accuracy: 0.9236\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2763 - accuracy: 0.9229 - val_loss: 0.2750 - val_accuracy: 0.9236\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.9215\n",
      "\n",
      "Test score: 0.2774553894996643\n",
      "Test accuracy: 0.921500027179718\n"
     ]
    }
   ],
   "source": [
    "# Import dos Pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  \n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "OPTIMIZER = SGD() # otimizador SGD\n",
    "N_HIDDEN = 128  # número de neurônios ocultos\n",
    "VALIDATION_SPLIT = 0.2 # quanto é reservado para validação\n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "# A camada final usa a função de ativação softmax, que é uma generalização da função sigmóide. \n",
    "# Softmax transforma um vetor k-dimensional de valores reais arbitrários em um vetor k-dimensional de valores reais no \n",
    "# intervalo (0, 1). No nosso caso, agrega 10 respostas fornecidas pela camada anterior com 10 neurônios.\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "# Precisamos selecionar o otimizador que é o algoritmo específico usado para atualizar pesos enquanto \n",
    "# treinamos nosso modelo.\n",
    "# Precisamos selecionar também a função objetivo que é usada pelo otimizador para navegar no espaço de pesos \n",
    "# (frequentemente, as funções objetivo são chamadas de função de perda (loss) e o processo de otimização é definido \n",
    "# como um processo de minimização de perdas).\n",
    "# Outras funções aqui: https://keras.io/losses/\n",
    "# A função objetivo \"categorical_crossentropy\" é a função objetivo adequada para predições de rótulos multiclass. \n",
    "# É também a escolha padrão em associação com a ativação softmax.\n",
    "# A métrica é usada para medir a performance do modelo. Outras métricas: https://keras.io/metrics/\n",
    "# As métricas são semelhantes às funções objetivo, com a única diferença de que elas não são usadas para \n",
    "# treinar um modelo, mas apenas para avaliar um modelo. \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "# Epochs: Este é o número de vezes que o modelo é exposto ao conjunto de treinamento. Em cada iteração, \n",
    "# o otimizador tenta ajustar os pesos para que a função objetivo seja minimizada. \n",
    "# Batch_size: Esse é o número de instâncias de treinamento observadas antes que o otimizador execute uma \n",
    "# atualização de peso.\n",
    "# Reservamos parte do conjunto de treinamento para validação. A idéia chave é que reservamos uma parte dos \n",
    "# dados de treinamento para medir o desempenho na validação durante o treinamento. \n",
    "# Esta é uma boa prática para qualquer tarefa de aprendizagem da máquina, que vamos adotar em todos os nossos exemplos.\n",
    "modelo_v1 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "# Uma vez treinado o modelo, podemos avaliá-lo no conjunto de testes que contém novos exemplos não vistos. \n",
    "# Desta forma, podemos obter o valor mínimo alcançado pela função objetivo e o melhor valor alcançado pela métrica \n",
    "# de avaliação. Note-se que o conjunto de treinamento e o conjunto de teste são rigorosamente separados. \n",
    "# Não vale a pena avaliar um modelo em um exemplo que já foi usado para treinamento. \n",
    "# A aprendizagem é essencialmente um processo destinado a generalizar observações invisíveis e não a memorizar \n",
    "# o que já é conhecido.\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adiciona 2 camadas ocultas, usando função de ativação ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treino\n",
      "10000 exemplos de teste\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.3792 - accuracy: 0.6646 - val_loss: 0.6930 - val_accuracy: 0.8400\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5722 - accuracy: 0.8562 - val_loss: 0.4417 - val_accuracy: 0.8860\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4316 - accuracy: 0.8832 - val_loss: 0.3694 - val_accuracy: 0.8986\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3763 - accuracy: 0.8951 - val_loss: 0.3337 - val_accuracy: 0.9064\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3444 - accuracy: 0.9028 - val_loss: 0.3115 - val_accuracy: 0.9118\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3216 - accuracy: 0.9083 - val_loss: 0.2947 - val_accuracy: 0.9180\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3043 - accuracy: 0.9134 - val_loss: 0.2805 - val_accuracy: 0.9196\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2902 - accuracy: 0.9172 - val_loss: 0.2700 - val_accuracy: 0.9227\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2778 - accuracy: 0.9205 - val_loss: 0.2596 - val_accuracy: 0.9257\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2668 - accuracy: 0.9237 - val_loss: 0.2509 - val_accuracy: 0.9268\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2567 - accuracy: 0.9264 - val_loss: 0.2430 - val_accuracy: 0.9298\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2476 - accuracy: 0.9289 - val_loss: 0.2351 - val_accuracy: 0.9314\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2393 - accuracy: 0.9316 - val_loss: 0.2293 - val_accuracy: 0.9333\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2317 - accuracy: 0.9340 - val_loss: 0.2225 - val_accuracy: 0.9349\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2243 - accuracy: 0.9359 - val_loss: 0.2167 - val_accuracy: 0.9371\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2175 - accuracy: 0.9377 - val_loss: 0.2130 - val_accuracy: 0.9377\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2112 - accuracy: 0.9396 - val_loss: 0.2065 - val_accuracy: 0.9397\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2054 - accuracy: 0.9409 - val_loss: 0.2019 - val_accuracy: 0.9423\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1999 - accuracy: 0.9427 - val_loss: 0.1976 - val_accuracy: 0.9438\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.1946 - accuracy: 0.9444 - val_loss: 0.1930 - val_accuracy: 0.9450\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9437\n",
      "\n",
      "Test score: 0.1920061856508255\n",
      "Test accuracy: 0.9437000155448914\n"
     ]
    }
   ],
   "source": [
    "# Import dos Pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  \n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "OPTIMIZER = SGD() # otimizador SGD\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # quanto é reservado para validação\n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "modelo_v2 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando Dropout nas camadas ocultas e aumentando o número de epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A segunda melhoria é muito simples. Decidimos retirar aleatoriamente, alguns dos valores propagados dentro das camadas  internas densas (camadas ocultas). Na aprendizagem de máquina, esta é uma forma bem conhecida de regularização. Surpreendentemente, esta ideia de descartar aleatoriamente alguns valores pode melhorar nosso desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout é uma técnica onde os neurônios selecionados aleatoriamente são ignorados durante o treinamento. Eles são \"abandonados\" aleatoriamente. Isto significa que a sua contribuição para a ativação de neurónios é removida temporariamente na passagem para a frente e quaisquer atualizações de peso não são aplicadas ao neurônio na passagem para trás."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode imaginar que, se os neurônios forem descartados aleatoriamente da rede durante o treinamento, outros neurônios terão de intervir e lidar com a representação necessária para fazer previsões para os neurônios ausentes. Acredita-se que isso resulte em múltiplas representações internas independentes sendo aprendidas pela rede.\n",
    "\n",
    "O efeito é que a rede se torna menos sensível aos pesos específicos dos neurônios. Isso, por sua vez resulta em uma rede que é capaz de melhorar a generalização e é menos sensível ao overfitting nos dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dropout é facilmente implementado, por seleção aleatória de neurônios a abandonar, com uma dada probabilidade (por exemplo, 20%) a cada ciclo de atualização de peso. Dropout é usado somente durante o treinamento de um modelo e não é usado ao avaliar a habilidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geralmente, usamos valores de 20% - 50% de neurônios para Dropout, com 20% fornecendo um bom ponto de partida. Uma probabilidade muito baixa tem efeito mínimo e um valor muito alto resulta em sub-aprendizagem pela rede. É provável que você obtenha melhor desempenho quando o Dropout é usado em uma rede maior, dando ao modelo mais de uma oportunidade de aprender representações independentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treino\n",
      "10000 exemplos de teste\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.7369 - accuracy: 0.4373 - val_loss: 0.9510 - val_accuracy: 0.8045\n",
      "Epoch 2/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.9617 - accuracy: 0.7084 - val_loss: 0.5519 - val_accuracy: 0.8675\n",
      "Epoch 3/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.7150 - accuracy: 0.7822 - val_loss: 0.4362 - val_accuracy: 0.8875\n",
      "Epoch 4/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.8186 - val_loss: 0.3808 - val_accuracy: 0.8976\n",
      "Epoch 5/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.5428 - accuracy: 0.8393 - val_loss: 0.3448 - val_accuracy: 0.9052\n",
      "Epoch 6/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.8528 - val_loss: 0.3206 - val_accuracy: 0.9112\n",
      "Epoch 7/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4693 - accuracy: 0.8604 - val_loss: 0.3023 - val_accuracy: 0.9148\n",
      "Epoch 8/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.4382 - accuracy: 0.8703 - val_loss: 0.2882 - val_accuracy: 0.9181\n",
      "Epoch 9/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.4208 - accuracy: 0.8759 - val_loss: 0.2739 - val_accuracy: 0.9224\n",
      "Epoch 10/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3994 - accuracy: 0.8825 - val_loss: 0.2640 - val_accuracy: 0.9236\n",
      "Epoch 11/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3861 - accuracy: 0.8862 - val_loss: 0.2541 - val_accuracy: 0.9268\n",
      "Epoch 12/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3686 - accuracy: 0.8917 - val_loss: 0.2441 - val_accuracy: 0.9305\n",
      "Epoch 13/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3584 - accuracy: 0.8942 - val_loss: 0.2363 - val_accuracy: 0.9319\n",
      "Epoch 14/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8982 - val_loss: 0.2288 - val_accuracy: 0.9341\n",
      "Epoch 15/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3351 - accuracy: 0.9026 - val_loss: 0.2232 - val_accuracy: 0.9353\n",
      "Epoch 16/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3276 - accuracy: 0.9054 - val_loss: 0.2163 - val_accuracy: 0.9373\n",
      "Epoch 17/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3172 - accuracy: 0.9066 - val_loss: 0.2097 - val_accuracy: 0.9393\n",
      "Epoch 18/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.3078 - accuracy: 0.9093 - val_loss: 0.2046 - val_accuracy: 0.9402\n",
      "Epoch 19/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.3008 - accuracy: 0.9122 - val_loss: 0.1996 - val_accuracy: 0.9420\n",
      "Epoch 20/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.9147 - val_loss: 0.1949 - val_accuracy: 0.9437\n",
      "Epoch 21/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2885 - accuracy: 0.9170 - val_loss: 0.1912 - val_accuracy: 0.9448\n",
      "Epoch 22/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2801 - accuracy: 0.9183 - val_loss: 0.1865 - val_accuracy: 0.9455\n",
      "Epoch 23/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2741 - accuracy: 0.9193 - val_loss: 0.1828 - val_accuracy: 0.9462\n",
      "Epoch 24/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2701 - accuracy: 0.9209 - val_loss: 0.1792 - val_accuracy: 0.9470\n",
      "Epoch 25/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2623 - accuracy: 0.9239 - val_loss: 0.1754 - val_accuracy: 0.9482\n",
      "Epoch 26/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2577 - accuracy: 0.9252 - val_loss: 0.1719 - val_accuracy: 0.9494\n",
      "Epoch 27/250\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.2550 - accuracy: 0.9245 - val_loss: 0.1692 - val_accuracy: 0.9498\n",
      "Epoch 28/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2480 - accuracy: 0.9262 - val_loss: 0.1672 - val_accuracy: 0.9515\n",
      "Epoch 29/250\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.2453 - accuracy: 0.9286 - val_loss: 0.1644 - val_accuracy: 0.9527\n",
      "Epoch 30/250\n",
      "352/375 [===========================>..] - ETA: 0s - loss: 0.2398 - accuracy: 0.9291"
     ]
    }
   ],
   "source": [
    "# Import dos pacotes\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  \n",
    "\n",
    "# Parâmetros da rede e do treinamento\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "OPTIMIZER = SGD() # otimizador SGD\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # quanto é reservado para validação\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Gerando datasets de treino e e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "# Gera versão final dos datasetes de treino e de teste\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treino')\n",
    "print(X_test.shape[0], 'exemplos de teste')\n",
    "\n",
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Cria as camadas\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Sumário\n",
    "model.summary()\n",
    "\n",
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\n",
    "\n",
    "# Treina o modelo\n",
    "modelo_v3 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "# Avalia o modelo com os dados de teste\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "\n",
    "# Imprime a perda e a acurácia\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Imprime os dados\n",
    "print(modelo_v3.history.keys())\n",
    "\n",
    "# Sumariza o modelo para acurácia\n",
    "plt.plot(modelo_v3.history['accuracy'])\n",
    "plt.plot(modelo_v3.history['val_accuracy'])\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "# Imprime a evolução de erro do modelo\n",
    "plt.plot(modelo_v3.history['loss'])\n",
    "plt.plot(modelo_v3.history['val_loss'])\n",
    "plt.title('Perda do Modelo')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.871999571164449,\n",
       "  0.521393892288208,\n",
       "  0.4177523496945699,\n",
       "  0.366813898007075,\n",
       "  0.334132959206899,\n",
       "  0.3102873472372691,\n",
       "  0.291970361550649,\n",
       "  0.27642452088991804,\n",
       "  0.2644186542828878,\n",
       "  0.25272273087501523,\n",
       "  0.24256080039342245,\n",
       "  0.23384717671076458,\n",
       "  0.22629604335625966,\n",
       "  0.219956143339475,\n",
       "  0.21347696204980215,\n",
       "  0.20682338778177897,\n",
       "  0.20141988857587179,\n",
       "  0.19655210212866464,\n",
       "  0.19131107012430826,\n",
       "  0.1875295756260554,\n",
       "  0.183781037569046,\n",
       "  0.1788161812822024,\n",
       "  0.1756703089872996,\n",
       "  0.17358675563335418,\n",
       "  0.1695327783425649,\n",
       "  0.16662849358717602,\n",
       "  0.16444878935813903,\n",
       "  0.1610063178539276,\n",
       "  0.1583196446299553,\n",
       "  0.1559868117570877,\n",
       "  0.15351283091306686,\n",
       "  0.15172955838839214,\n",
       "  0.14920109055439632,\n",
       "  0.1476012070775032,\n",
       "  0.14518754879633586,\n",
       "  0.14400072451432547,\n",
       "  0.14210421262184778,\n",
       "  0.14050010921557746,\n",
       "  0.13913086926937104,\n",
       "  0.13798205932974816,\n",
       "  0.13554945316910744,\n",
       "  0.13374143987894058,\n",
       "  0.13215290693442028,\n",
       "  0.13125316510597865,\n",
       "  0.12966657936573028,\n",
       "  0.12896193359295527,\n",
       "  0.12731600893537204,\n",
       "  0.12710221207141875,\n",
       "  0.12528126028180123,\n",
       "  0.12429961670438448,\n",
       "  0.12264348325133324,\n",
       "  0.12224031128485997,\n",
       "  0.12073187904556593,\n",
       "  0.11968625209728877,\n",
       "  0.11932881804307302,\n",
       "  0.11853465887904167,\n",
       "  0.11721504775683085,\n",
       "  0.11632991524537405,\n",
       "  0.11575032306710879,\n",
       "  0.11451062297821045,\n",
       "  0.11347799201806387,\n",
       "  0.11311590509613355,\n",
       "  0.11225592822829883,\n",
       "  0.11204802170395851,\n",
       "  0.11141155101855596,\n",
       "  0.11080316623051961,\n",
       "  0.1100211744705836,\n",
       "  0.10854206881920496,\n",
       "  0.10816381444533665,\n",
       "  0.10730853312214216,\n",
       "  0.10721798832217852,\n",
       "  0.1062190254231294,\n",
       "  0.10546970680356026,\n",
       "  0.10541149657964706,\n",
       "  0.10498425792654355,\n",
       "  0.10435533172885576,\n",
       "  0.10398757378260294,\n",
       "  0.1032457653582096,\n",
       "  0.10351066269477209,\n",
       "  0.10340700980027517,\n",
       "  0.10217260386546453,\n",
       "  0.10149867176512876,\n",
       "  0.10114230520526568,\n",
       "  0.1008902683854103,\n",
       "  0.10016246782243252,\n",
       "  0.10017216549317041,\n",
       "  0.09950701134403547,\n",
       "  0.09935394202669462,\n",
       "  0.09831198950608572,\n",
       "  0.09839140337208907,\n",
       "  0.09773367715875307,\n",
       "  0.09725434537231922,\n",
       "  0.09670438240468503,\n",
       "  0.0964294031659762,\n",
       "  0.09665178030232588,\n",
       "  0.09674161565303803,\n",
       "  0.09595397659142813,\n",
       "  0.09557876309752464,\n",
       "  0.09565312991539637,\n",
       "  0.09525139264265696,\n",
       "  0.0951430862993002,\n",
       "  0.09473571855823198,\n",
       "  0.09431856875618298,\n",
       "  0.09383948795000711,\n",
       "  0.09307371932268142,\n",
       "  0.09351328938702742,\n",
       "  0.09394393743077914,\n",
       "  0.09269869487981001,\n",
       "  0.09266713130970795,\n",
       "  0.09171983480453491,\n",
       "  0.09179124609629313,\n",
       "  0.09148975134889285,\n",
       "  0.09198786441981792,\n",
       "  0.09094709227482478,\n",
       "  0.09078665681680044,\n",
       "  0.09115670782327652,\n",
       "  0.09086098858217398,\n",
       "  0.08995543293158213,\n",
       "  0.09042837238311767,\n",
       "  0.0897579378883044,\n",
       "  0.09070249961813291,\n",
       "  0.08939326633512974,\n",
       "  0.08891990407804648,\n",
       "  0.08895500252644221,\n",
       "  0.08844711043437321,\n",
       "  0.08890153580407302,\n",
       "  0.08883088102191687,\n",
       "  0.08797655034561952,\n",
       "  0.08828698416550954,\n",
       "  0.08812591836353142,\n",
       "  0.08726821153114239,\n",
       "  0.08746123513579368,\n",
       "  0.08745246188342572,\n",
       "  0.0874534050921599,\n",
       "  0.08760325860480467,\n",
       "  0.0867525751615564,\n",
       "  0.0871443738763531,\n",
       "  0.08624127382040024,\n",
       "  0.08640982583910227,\n",
       "  0.08618162447959185,\n",
       "  0.08546229117612043,\n",
       "  0.08653614789744218,\n",
       "  0.0850354366923372,\n",
       "  0.08560640543450912,\n",
       "  0.08530568268398443,\n",
       "  0.08479671497891346,\n",
       "  0.08481078511973222,\n",
       "  0.08504357348630825,\n",
       "  0.08484776891767978,\n",
       "  0.08460316480944555,\n",
       "  0.08514290590832631,\n",
       "  0.0844974313179652,\n",
       "  0.08373664919535319,\n",
       "  0.0840012474283576,\n",
       "  0.0841077196498712,\n",
       "  0.08390415182709694,\n",
       "  0.08381083021809657,\n",
       "  0.08425166887044906,\n",
       "  0.08341817345966895,\n",
       "  0.08373044682045777,\n",
       "  0.0834661357825001,\n",
       "  0.08333617933094502,\n",
       "  0.0834838119794925,\n",
       "  0.08338604325304429,\n",
       "  0.08306602309892575,\n",
       "  0.08268544911841551,\n",
       "  0.08242951378971339,\n",
       "  0.08229198147108158,\n",
       "  0.08242523265381654,\n",
       "  0.08203628928462664,\n",
       "  0.08196946398168803,\n",
       "  0.08245639952768882,\n",
       "  0.08241042739152908,\n",
       "  0.08180828765034676,\n",
       "  0.08201088424772024,\n",
       "  0.08240918418516716,\n",
       "  0.08210501572489738,\n",
       "  0.08182147379467884,\n",
       "  0.08167593588183324,\n",
       "  0.08216000298162301,\n",
       "  0.08203366198639075,\n",
       "  0.08205978883554538,\n",
       "  0.0816541764959693,\n",
       "  0.08099196362743775,\n",
       "  0.08170367256800333,\n",
       "  0.08152422965317964,\n",
       "  0.08136748912433783,\n",
       "  0.08083695046852032,\n",
       "  0.08117236944039663,\n",
       "  0.08127109279980262,\n",
       "  0.08151057178278764,\n",
       "  0.08082127304375172,\n",
       "  0.08104372343669335,\n",
       "  0.08067285862316688,\n",
       "  0.08076616988827785,\n",
       "  0.08112569032857815,\n",
       "  0.08064376109590134,\n",
       "  0.08109351912140847,\n",
       "  0.08058689156795541,\n",
       "  0.08046470529461901,\n",
       "  0.08033855028326313,\n",
       "  0.08042616600791613,\n",
       "  0.08025285766770442,\n",
       "  0.08059642961000403,\n",
       "  0.07971639031171798,\n",
       "  0.0799047132184108,\n",
       "  0.08070861812432607,\n",
       "  0.08073358227560917,\n",
       "  0.0801022519953549,\n",
       "  0.07997565781325101,\n",
       "  0.08047341053684552,\n",
       "  0.07989382061113914,\n",
       "  0.07960958785067002,\n",
       "  0.0798661517004172,\n",
       "  0.07940204356610775,\n",
       "  0.07976739821210503,\n",
       "  0.07971468136956295,\n",
       "  0.0800717469677329,\n",
       "  0.07909792777771751,\n",
       "  0.07891608283668756,\n",
       "  0.07887482927491267,\n",
       "  0.07957956936458746,\n",
       "  0.07897949682672818,\n",
       "  0.0787984209333857,\n",
       "  0.0786894328246514,\n",
       "  0.07903080170104901,\n",
       "  0.07890559161702791,\n",
       "  0.07882043961435556,\n",
       "  0.0792345328728358,\n",
       "  0.07916343415280183,\n",
       "  0.07886880462244153,\n",
       "  0.07864963394403458,\n",
       "  0.07812097280224164,\n",
       "  0.07788025477528572,\n",
       "  0.07760061508417129,\n",
       "  0.07909349785000086,\n",
       "  0.07860834622383117,\n",
       "  0.0788097498814265,\n",
       "  0.07893218853200476,\n",
       "  0.07810796315222979,\n",
       "  0.07799393801391125,\n",
       "  0.07831159827361504,\n",
       "  0.07821189290533463,\n",
       "  0.078396965538462,\n",
       "  0.07866764760638277,\n",
       "  0.078483530467997,\n",
       "  0.07933228862037262,\n",
       "  0.0789734168301026,\n",
       "  0.0785114310408632,\n",
       "  0.07845981373389561],\n",
       " 'val_accuracy': [0.809499979019165,\n",
       "  0.8644166588783264,\n",
       "  0.8890833258628845,\n",
       "  0.9001666903495789,\n",
       "  0.906333327293396,\n",
       "  0.9129999876022339,\n",
       "  0.9168333411216736,\n",
       "  0.922166645526886,\n",
       "  0.9253333210945129,\n",
       "  0.9279166460037231,\n",
       "  0.9304999709129333,\n",
       "  0.9328333139419556,\n",
       "  0.9353333115577698,\n",
       "  0.9365000128746033,\n",
       "  0.9387500286102295,\n",
       "  0.940500020980835,\n",
       "  0.9420833587646484,\n",
       "  0.9434166550636292,\n",
       "  0.9455833435058594,\n",
       "  0.9467499852180481,\n",
       "  0.9481666684150696,\n",
       "  0.9495000243186951,\n",
       "  0.9506666660308838,\n",
       "  0.9516666531562805,\n",
       "  0.95291668176651,\n",
       "  0.953166663646698,\n",
       "  0.9543333053588867,\n",
       "  0.9545833468437195,\n",
       "  0.9557499885559082,\n",
       "  0.9567499756813049,\n",
       "  0.9573333263397217,\n",
       "  0.9572499990463257,\n",
       "  0.9574166536331177,\n",
       "  0.9574166536331177,\n",
       "  0.9587500095367432,\n",
       "  0.9588333368301392,\n",
       "  0.9592499732971191,\n",
       "  0.9593333601951599,\n",
       "  0.9599166512489319,\n",
       "  0.9602500200271606,\n",
       "  0.9610833525657654,\n",
       "  0.9619166851043701,\n",
       "  0.9617499709129333,\n",
       "  0.9620000123977661,\n",
       "  0.9628333449363708,\n",
       "  0.9628333449363708,\n",
       "  0.9631666541099548,\n",
       "  0.9629166722297668,\n",
       "  0.9635833501815796,\n",
       "  0.9632499814033508,\n",
       "  0.9640833139419556,\n",
       "  0.9640833139419556,\n",
       "  0.9645000100135803,\n",
       "  0.9653333425521851,\n",
       "  0.9653333425521851,\n",
       "  0.965416669845581,\n",
       "  0.965749979019165,\n",
       "  0.965749979019165,\n",
       "  0.9660833477973938,\n",
       "  0.9669166803359985,\n",
       "  0.9668333530426025,\n",
       "  0.9668333530426025,\n",
       "  0.9672499895095825,\n",
       "  0.9672499895095825,\n",
       "  0.9673333168029785,\n",
       "  0.9673333168029785,\n",
       "  0.9677500128746033,\n",
       "  0.968583345413208,\n",
       "  0.9676666855812073,\n",
       "  0.968500018119812,\n",
       "  0.96875,\n",
       "  0.968833327293396,\n",
       "  0.9693333506584167,\n",
       "  0.9695833325386047,\n",
       "  0.968999981880188,\n",
       "  0.9695000052452087,\n",
       "  0.9695000052452087,\n",
       "  0.9695833325386047,\n",
       "  0.9695000052452087,\n",
       "  0.968916654586792,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9709166884422302,\n",
       "  0.9703333377838135,\n",
       "  0.9701666831970215,\n",
       "  0.9700833559036255,\n",
       "  0.9712499976158142,\n",
       "  0.9704999923706055,\n",
       "  0.9705833196640015,\n",
       "  0.9710000157356262,\n",
       "  0.9714166522026062,\n",
       "  0.971750020980835,\n",
       "  0.9711666703224182,\n",
       "  0.9714999794960022,\n",
       "  0.9712499976158142,\n",
       "  0.9714166522026062,\n",
       "  0.972000002861023,\n",
       "  0.972000002861023,\n",
       "  0.971916675567627,\n",
       "  0.971833348274231,\n",
       "  0.9711666703224182,\n",
       "  0.971916675567627,\n",
       "  0.972083330154419,\n",
       "  0.971916675567627,\n",
       "  0.9721666574478149,\n",
       "  0.971750020980835,\n",
       "  0.971666693687439,\n",
       "  0.9722499847412109,\n",
       "  0.9721666574478149,\n",
       "  0.9723333120346069,\n",
       "  0.9726666808128357,\n",
       "  0.9725833535194397,\n",
       "  0.9725000262260437,\n",
       "  0.9729999899864197,\n",
       "  0.9729166626930237,\n",
       "  0.9729166626930237,\n",
       "  0.9729166626930237,\n",
       "  0.9723333120346069,\n",
       "  0.9732499718666077,\n",
       "  0.9728333353996277,\n",
       "  0.9728333353996277,\n",
       "  0.9729166626930237,\n",
       "  0.9733333587646484,\n",
       "  0.9735000133514404,\n",
       "  0.9737499952316284,\n",
       "  0.9735833406448364,\n",
       "  0.9736666679382324,\n",
       "  0.9738333225250244,\n",
       "  0.9739166498184204,\n",
       "  0.9742500185966492,\n",
       "  0.9741666913032532,\n",
       "  0.9742500185966492,\n",
       "  0.9739166498184204,\n",
       "  0.9737499952316284,\n",
       "  0.9737499952316284,\n",
       "  0.9736666679382324,\n",
       "  0.9743333458900452,\n",
       "  0.9740833044052124,\n",
       "  0.9741666913032532,\n",
       "  0.9745000004768372,\n",
       "  0.9743333458900452,\n",
       "  0.9739166498184204,\n",
       "  0.9743333458900452,\n",
       "  0.9745000004768372,\n",
       "  0.9746666550636292,\n",
       "  0.9745000004768372,\n",
       "  0.9743333458900452,\n",
       "  0.9744166731834412,\n",
       "  0.9745833277702332,\n",
       "  0.9751666784286499,\n",
       "  0.9747499823570251,\n",
       "  0.9746666550636292,\n",
       "  0.9749166369438171,\n",
       "  0.9744166731834412,\n",
       "  0.9749166369438171,\n",
       "  0.9747499823570251,\n",
       "  0.9745000004768372,\n",
       "  0.9751666784286499,\n",
       "  0.9752500057220459,\n",
       "  0.9754166603088379,\n",
       "  0.9751666784286499,\n",
       "  0.9753333330154419,\n",
       "  0.9751666784286499,\n",
       "  0.9752500057220459,\n",
       "  0.9748333096504211,\n",
       "  0.9755833148956299,\n",
       "  0.9753333330154419,\n",
       "  0.9758333563804626,\n",
       "  0.9754166603088379,\n",
       "  0.9754166603088379,\n",
       "  0.9756666421890259,\n",
       "  0.9751666784286499,\n",
       "  0.9759166836738586,\n",
       "  0.9756666421890259,\n",
       "  0.9760833382606506,\n",
       "  0.9765833616256714,\n",
       "  0.9759166836738586,\n",
       "  0.9758333563804626,\n",
       "  0.9760833382606506,\n",
       "  0.9762499928474426,\n",
       "  0.9760833382606506,\n",
       "  0.9760833382606506,\n",
       "  0.9764166474342346,\n",
       "  0.9759166836738586,\n",
       "  0.9757500290870667,\n",
       "  0.9758333563804626,\n",
       "  0.9773333072662354,\n",
       "  0.9766666889190674,\n",
       "  0.9764999747276306,\n",
       "  0.9759166836738586,\n",
       "  0.9766666889190674,\n",
       "  0.9765833616256714,\n",
       "  0.9764166474342346,\n",
       "  0.9765833616256714,\n",
       "  0.9768333435058594,\n",
       "  0.9762499928474426,\n",
       "  0.9763333201408386,\n",
       "  0.9762499928474426,\n",
       "  0.9765833616256714,\n",
       "  0.9765833616256714,\n",
       "  0.9768333435058594,\n",
       "  0.9766666889190674,\n",
       "  0.9763333201408386,\n",
       "  0.9769166707992554,\n",
       "  0.9770833253860474,\n",
       "  0.9769166707992554,\n",
       "  0.9773333072662354,\n",
       "  0.9769166707992554,\n",
       "  0.9764999747276306,\n",
       "  0.9767500162124634,\n",
       "  0.9767500162124634,\n",
       "  0.9767500162124634,\n",
       "  0.9768333435058594,\n",
       "  0.9773333072662354,\n",
       "  0.9772499799728394,\n",
       "  0.9772499799728394,\n",
       "  0.9774166941642761,\n",
       "  0.9765833616256714,\n",
       "  0.9775000214576721,\n",
       "  0.9766666889190674,\n",
       "  0.9771666526794434,\n",
       "  0.9764999747276306,\n",
       "  0.9765833616256714,\n",
       "  0.9769999980926514,\n",
       "  0.9773333072662354,\n",
       "  0.9768333435058594,\n",
       "  0.9772499799728394,\n",
       "  0.9775833487510681,\n",
       "  0.9769999980926514,\n",
       "  0.9772499799728394,\n",
       "  0.9775000214576721,\n",
       "  0.9773333072662354,\n",
       "  0.9774166941642761,\n",
       "  0.9778333306312561,\n",
       "  0.9775833487510681,\n",
       "  0.9771666526794434,\n",
       "  0.9776666760444641,\n",
       "  0.9775000214576721,\n",
       "  0.9775000214576721,\n",
       "  0.9778333306312561,\n",
       "  0.9770833253860474,\n",
       "  0.9775000214576721,\n",
       "  0.9772499799728394,\n",
       "  0.9769166707992554,\n",
       "  0.9772499799728394,\n",
       "  0.9775833487510681,\n",
       "  0.9777500033378601,\n",
       "  0.9769999980926514,\n",
       "  0.9772499799728394,\n",
       "  0.9769166707992554],\n",
       " 'loss': [1.6718722133636474,\n",
       "  0.8992075864473978,\n",
       "  0.683758979956309,\n",
       "  0.5846770370006561,\n",
       "  0.525754035393397,\n",
       "  0.4846093362967173,\n",
       "  0.45095719655354816,\n",
       "  0.4250658965110779,\n",
       "  0.4066349815527598,\n",
       "  0.3888261545499166,\n",
       "  0.3689992209275564,\n",
       "  0.3565222722689311,\n",
       "  0.3460938101609548,\n",
       "  0.3337863127787908,\n",
       "  0.32272762779394787,\n",
       "  0.3126862561305364,\n",
       "  0.30328299188613894,\n",
       "  0.29707930596669513,\n",
       "  0.2882234175602595,\n",
       "  0.28424983847141266,\n",
       "  0.27636587715148925,\n",
       "  0.27229393323262535,\n",
       "  0.26472316960493725,\n",
       "  0.2597511806686719,\n",
       "  0.2584855343103409,\n",
       "  0.25101329493522645,\n",
       "  0.2425371769865354,\n",
       "  0.24031073115269344,\n",
       "  0.23818535254398981,\n",
       "  0.2354887643257777,\n",
       "  0.22931666777531307,\n",
       "  0.22473954061667126,\n",
       "  0.22363058576981226,\n",
       "  0.21963726669549943,\n",
       "  0.21812078628937404,\n",
       "  0.21508833760023116,\n",
       "  0.20973961927493412,\n",
       "  0.2089105509320895,\n",
       "  0.2067344210743904,\n",
       "  0.20192610003550848,\n",
       "  0.20133131754398345,\n",
       "  0.197442391594251,\n",
       "  0.1954077922105789,\n",
       "  0.19263087328275044,\n",
       "  0.19301348110040029,\n",
       "  0.18711576068401337,\n",
       "  0.18324474883079528,\n",
       "  0.18353739873568217,\n",
       "  0.1814186001618703,\n",
       "  0.1808385282754898,\n",
       "  0.18043143474062284,\n",
       "  0.17996450672547024,\n",
       "  0.17542964530984562,\n",
       "  0.17495988535881044,\n",
       "  0.16999358701705933,\n",
       "  0.1710246942639351,\n",
       "  0.16967929166555404,\n",
       "  0.16578040849169096,\n",
       "  0.16689300194382667,\n",
       "  0.1639009438554446,\n",
       "  0.15957941029469172,\n",
       "  0.1611165189643701,\n",
       "  0.1581169713238875,\n",
       "  0.15694060337543486,\n",
       "  0.15679801674683888,\n",
       "  0.15497116895516713,\n",
       "  0.1547809205551942,\n",
       "  0.15250685479243598,\n",
       "  0.15094121143221856,\n",
       "  0.151208409935236,\n",
       "  0.148521662324667,\n",
       "  0.1467129402856032,\n",
       "  0.1472895431915919,\n",
       "  0.14295105333129565,\n",
       "  0.14402254631121952,\n",
       "  0.14557483849922817,\n",
       "  0.14220314116279284,\n",
       "  0.14022393003106118,\n",
       "  0.14061547034978866,\n",
       "  0.14065883350372316,\n",
       "  0.13738761062423388,\n",
       "  0.1358161683579286,\n",
       "  0.13489491721987723,\n",
       "  0.13663298514485359,\n",
       "  0.13447861076394718,\n",
       "  0.13098290048042932,\n",
       "  0.13097946566343308,\n",
       "  0.12941854948798814,\n",
       "  0.13131663462519647,\n",
       "  0.1285410581032435,\n",
       "  0.12787239705522854,\n",
       "  0.1299058807293574,\n",
       "  0.1256942524413268,\n",
       "  0.12515774271885555,\n",
       "  0.12730577665567397,\n",
       "  0.1263186510403951,\n",
       "  0.12174668548504511,\n",
       "  0.12283007582028707,\n",
       "  0.11944881317019462,\n",
       "  0.12215305808186531,\n",
       "  0.1229182566901048,\n",
       "  0.11743703959385554,\n",
       "  0.119705665320158,\n",
       "  0.11683507142961025,\n",
       "  0.1174767566372951,\n",
       "  0.1143974838455518,\n",
       "  0.11542057522634665,\n",
       "  0.11316583225131036,\n",
       "  0.11293486032883326,\n",
       "  0.11509303732713064,\n",
       "  0.11660770687957604,\n",
       "  0.11234165783723195,\n",
       "  0.112706650664409,\n",
       "  0.11186664322018623,\n",
       "  0.11055477420488993,\n",
       "  0.11099348080158233,\n",
       "  0.10879751584927241,\n",
       "  0.10865701960523923,\n",
       "  0.10822419410943986,\n",
       "  0.1066157892694076,\n",
       "  0.10832147735357285,\n",
       "  0.10390481857955455,\n",
       "  0.1061399976114432,\n",
       "  0.10567533836762111,\n",
       "  0.10591114595532418,\n",
       "  0.10494920891523361,\n",
       "  0.10436182590325674,\n",
       "  0.10409288234512011,\n",
       "  0.10528836751977602,\n",
       "  0.10376010650396347,\n",
       "  0.10161979921658834,\n",
       "  0.10025311340888342,\n",
       "  0.09912813422083855,\n",
       "  0.10144342001279195,\n",
       "  0.09871464263647795,\n",
       "  0.0971934193422397,\n",
       "  0.09625182654460271,\n",
       "  0.0979212400764227,\n",
       "  0.09941119007766247,\n",
       "  0.09649601362148921,\n",
       "  0.09714528621236483,\n",
       "  0.09605762114624182,\n",
       "  0.09550403873125712,\n",
       "  0.0929778832346201,\n",
       "  0.0933005146831274,\n",
       "  0.09484709826608499,\n",
       "  0.09463901118934155,\n",
       "  0.09536252125104268,\n",
       "  0.09287844583888849,\n",
       "  0.09252023696402709,\n",
       "  0.09237083211541176,\n",
       "  0.09379832968115806,\n",
       "  0.09071931028862794,\n",
       "  0.09068792542815209,\n",
       "  0.0906665573467811,\n",
       "  0.08920762491722901,\n",
       "  0.08905718411008517,\n",
       "  0.08797566772003969,\n",
       "  0.08757262125611305,\n",
       "  0.08656840939323107,\n",
       "  0.08869213685890039,\n",
       "  0.08717559082309405,\n",
       "  0.08742000564436118,\n",
       "  0.08584114600718021,\n",
       "  0.08685220082104206,\n",
       "  0.08815349231660366,\n",
       "  0.0866272531747818,\n",
       "  0.08486220383395751,\n",
       "  0.0844598376105229,\n",
       "  0.0855181660403808,\n",
       "  0.08645889274775982,\n",
       "  0.0833883265554905,\n",
       "  0.08563760234912236,\n",
       "  0.08452908683816591,\n",
       "  0.08594685267905394,\n",
       "  0.08299352122346561,\n",
       "  0.08228781862556935,\n",
       "  0.08202325454105934,\n",
       "  0.08033563554783663,\n",
       "  0.08199699421226979,\n",
       "  0.08107089652121068,\n",
       "  0.08044467855244875,\n",
       "  0.08036414330204328,\n",
       "  0.08196108164886634,\n",
       "  0.07945034506420294,\n",
       "  0.07820535088082155,\n",
       "  0.07795571701228618,\n",
       "  0.07788306624690691,\n",
       "  0.08017335980633894,\n",
       "  0.0783656632527709,\n",
       "  0.07476238685846329,\n",
       "  0.07687020907799402,\n",
       "  0.07942780070006847,\n",
       "  0.07643195123473803,\n",
       "  0.07532043168942133,\n",
       "  0.07512830511977275,\n",
       "  0.07538412436346213,\n",
       "  0.0780446699783206,\n",
       "  0.07379948956767718,\n",
       "  0.07616352656980356,\n",
       "  0.07439504619936148,\n",
       "  0.07241702577720086,\n",
       "  0.07213199892888467,\n",
       "  0.07546354597061873,\n",
       "  0.07496590634932121,\n",
       "  0.07570569114387035,\n",
       "  0.07286426049967606,\n",
       "  0.07110995235542457,\n",
       "  0.07311770387987296,\n",
       "  0.07363068788250288,\n",
       "  0.0741514995098114,\n",
       "  0.07375964750349522,\n",
       "  0.07156639288862546,\n",
       "  0.0727189914509654,\n",
       "  0.07116568032403787,\n",
       "  0.07122272138794263,\n",
       "  0.07130595603585244,\n",
       "  0.06955583822975556,\n",
       "  0.07329069668302933,\n",
       "  0.07002548058331012,\n",
       "  0.07008433881898721,\n",
       "  0.07142518803477288,\n",
       "  0.0700193481867512,\n",
       "  0.0680681583210826,\n",
       "  0.07010175874332587,\n",
       "  0.06845714927713076,\n",
       "  0.06856252852082252,\n",
       "  0.06837873048832019,\n",
       "  0.06697360224028429,\n",
       "  0.06707507400214673,\n",
       "  0.06727642894039551,\n",
       "  0.06658514550328255,\n",
       "  0.06505235465615988,\n",
       "  0.06693866175413132,\n",
       "  0.06733325602610905,\n",
       "  0.0663282455901305,\n",
       "  0.06523035867015521,\n",
       "  0.06557061164329449,\n",
       "  0.06602629491190115,\n",
       "  0.06615507517258326,\n",
       "  0.0649811389297247,\n",
       "  0.06631172414372365,\n",
       "  0.06441112239658832,\n",
       "  0.06355551739782095,\n",
       "  0.06432605893413226,\n",
       "  0.06555197340995074,\n",
       "  0.06320356309662263,\n",
       "  0.06264182985574007,\n",
       "  0.0638159208148718,\n",
       "  0.06376772858202458],\n",
       " 'accuracy': [0.46670833,\n",
       "  0.7242708,\n",
       "  0.79141665,\n",
       "  0.8205625,\n",
       "  0.84141666,\n",
       "  0.85802084,\n",
       "  0.8670625,\n",
       "  0.87335414,\n",
       "  0.87916666,\n",
       "  0.88570833,\n",
       "  0.890875,\n",
       "  0.89472914,\n",
       "  0.89825,\n",
       "  0.9022292,\n",
       "  0.90489584,\n",
       "  0.907875,\n",
       "  0.91183335,\n",
       "  0.91170835,\n",
       "  0.91529167,\n",
       "  0.9170833,\n",
       "  0.91789585,\n",
       "  0.91897917,\n",
       "  0.92225,\n",
       "  0.9225417,\n",
       "  0.9246042,\n",
       "  0.92679167,\n",
       "  0.9297708,\n",
       "  0.9295625,\n",
       "  0.93008333,\n",
       "  0.930375,\n",
       "  0.93291664,\n",
       "  0.93308336,\n",
       "  0.9338958,\n",
       "  0.9345833,\n",
       "  0.93583333,\n",
       "  0.93689585,\n",
       "  0.937625,\n",
       "  0.93766665,\n",
       "  0.9392292,\n",
       "  0.94016665,\n",
       "  0.94075,\n",
       "  0.9412917,\n",
       "  0.9421875,\n",
       "  0.9430625,\n",
       "  0.9426875,\n",
       "  0.94472915,\n",
       "  0.94535416,\n",
       "  0.9450625,\n",
       "  0.9465,\n",
       "  0.94602084,\n",
       "  0.9459792,\n",
       "  0.94560415,\n",
       "  0.94825,\n",
       "  0.94783336,\n",
       "  0.9502083,\n",
       "  0.94872916,\n",
       "  0.95014584,\n",
       "  0.9506458,\n",
       "  0.95002085,\n",
       "  0.9517083,\n",
       "  0.9524375,\n",
       "  0.95185417,\n",
       "  0.95370835,\n",
       "  0.9537708,\n",
       "  0.9537708,\n",
       "  0.95435417,\n",
       "  0.9538958,\n",
       "  0.95435417,\n",
       "  0.954625,\n",
       "  0.9556875,\n",
       "  0.9566875,\n",
       "  0.9561875,\n",
       "  0.95566666,\n",
       "  0.95677084,\n",
       "  0.95708334,\n",
       "  0.9569167,\n",
       "  0.9571458,\n",
       "  0.9584375,\n",
       "  0.957375,\n",
       "  0.9580208,\n",
       "  0.9588125,\n",
       "  0.9590833,\n",
       "  0.959625,\n",
       "  0.959,\n",
       "  0.95935416,\n",
       "  0.96022916,\n",
       "  0.9601875,\n",
       "  0.96108335,\n",
       "  0.96020836,\n",
       "  0.96110415,\n",
       "  0.96089584,\n",
       "  0.960625,\n",
       "  0.962625,\n",
       "  0.96239585,\n",
       "  0.962125,\n",
       "  0.96254164,\n",
       "  0.9633125,\n",
       "  0.96272916,\n",
       "  0.964,\n",
       "  0.9632292,\n",
       "  0.9633333,\n",
       "  0.96477085,\n",
       "  0.9636875,\n",
       "  0.9658333,\n",
       "  0.9649792,\n",
       "  0.9658125,\n",
       "  0.96477085,\n",
       "  0.96608335,\n",
       "  0.9662708,\n",
       "  0.9655625,\n",
       "  0.96477085,\n",
       "  0.9668125,\n",
       "  0.9659167,\n",
       "  0.96620834,\n",
       "  0.96641666,\n",
       "  0.96585417,\n",
       "  0.967,\n",
       "  0.966875,\n",
       "  0.9672083,\n",
       "  0.9668958,\n",
       "  0.96695834,\n",
       "  0.96835417,\n",
       "  0.9685,\n",
       "  0.96847916,\n",
       "  0.9680833,\n",
       "  0.96814585,\n",
       "  0.96827084,\n",
       "  0.96847916,\n",
       "  0.96877086,\n",
       "  0.96802086,\n",
       "  0.96991664,\n",
       "  0.96914583,\n",
       "  0.9698125,\n",
       "  0.96920836,\n",
       "  0.969,\n",
       "  0.97064584,\n",
       "  0.97079164,\n",
       "  0.97066665,\n",
       "  0.96958333,\n",
       "  0.9713542,\n",
       "  0.97052085,\n",
       "  0.97083336,\n",
       "  0.97066665,\n",
       "  0.971,\n",
       "  0.9709375,\n",
       "  0.9713125,\n",
       "  0.9710625,\n",
       "  0.9704583,\n",
       "  0.97125,\n",
       "  0.971375,\n",
       "  0.9721875,\n",
       "  0.97154164,\n",
       "  0.97229165,\n",
       "  0.97216666,\n",
       "  0.97258335,\n",
       "  0.97314584,\n",
       "  0.97241664,\n",
       "  0.97272915,\n",
       "  0.9729375,\n",
       "  0.9739583,\n",
       "  0.97172916,\n",
       "  0.97370833,\n",
       "  0.973375,\n",
       "  0.9733958,\n",
       "  0.972375,\n",
       "  0.9732917,\n",
       "  0.9726667,\n",
       "  0.973625,\n",
       "  0.9735417,\n",
       "  0.973375,\n",
       "  0.9730833,\n",
       "  0.97445834,\n",
       "  0.97445834,\n",
       "  0.97335416,\n",
       "  0.9733125,\n",
       "  0.97391665,\n",
       "  0.97479165,\n",
       "  0.97464585,\n",
       "  0.9756875,\n",
       "  0.9753542,\n",
       "  0.975125,\n",
       "  0.97433335,\n",
       "  0.975375,\n",
       "  0.97510415,\n",
       "  0.97541666,\n",
       "  0.975375,\n",
       "  0.97577083,\n",
       "  0.97554165,\n",
       "  0.9754583,\n",
       "  0.9760417,\n",
       "  0.976875,\n",
       "  0.9759167,\n",
       "  0.97564584,\n",
       "  0.9759167,\n",
       "  0.97727084,\n",
       "  0.9759167,\n",
       "  0.9762292,\n",
       "  0.9750625,\n",
       "  0.9765,\n",
       "  0.9762083,\n",
       "  0.9764583,\n",
       "  0.9780833,\n",
       "  0.9779792,\n",
       "  0.97695833,\n",
       "  0.97716665,\n",
       "  0.97725,\n",
       "  0.97683334,\n",
       "  0.97727084,\n",
       "  0.97685415,\n",
       "  0.9773125,\n",
       "  0.9765,\n",
       "  0.976625,\n",
       "  0.9766875,\n",
       "  0.9775208,\n",
       "  0.9773125,\n",
       "  0.977,\n",
       "  0.9781875,\n",
       "  0.97802085,\n",
       "  0.9770833,\n",
       "  0.97783333,\n",
       "  0.978125,\n",
       "  0.97760415,\n",
       "  0.9785417,\n",
       "  0.977875,\n",
       "  0.9780833,\n",
       "  0.97816664,\n",
       "  0.978875,\n",
       "  0.9782708,\n",
       "  0.97975,\n",
       "  0.979375,\n",
       "  0.9788542,\n",
       "  0.9787083,\n",
       "  0.979375,\n",
       "  0.97891665,\n",
       "  0.97889584,\n",
       "  0.9797083,\n",
       "  0.97989583,\n",
       "  0.9796042,\n",
       "  0.97889584,\n",
       "  0.9798333,\n",
       "  0.97902083,\n",
       "  0.979375,\n",
       "  0.9794375,\n",
       "  0.9793958,\n",
       "  0.9799375,\n",
       "  0.9791458,\n",
       "  0.980125,\n",
       "  0.9807917,\n",
       "  0.9799375,\n",
       "  0.9803125]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_v3.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Versão 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando outros otimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma rede neural é essencialmente uma composição de múltiplas funções com milhares, e às vezes milhões, de parâmetros. Cada camada de rede calcula uma função cujo erro deve ser minimizado para melhorar a precisão observada durante a fase de aprendizagem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implementa uma variante rápida de descida de gradiente conhecida como descida de gradiente estocástica (SGD) e duas técnicas de otimização mais avançadas conhecidas como RMSprop e Adam. RMSprop e Adam incluem o conceito de momentum (uma componente de velocidade) além da componente de aceleração que tem SGD. Isso permite uma convergência mais rápida ao custo de mais computação. SGD foi nossa escolha padrão até agora. Então vamos tentar os outros dois. É muito simples, só precisamos mudar algumas linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dos pacotes e funções\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantindo que o resultado pode ser reproduzido\n",
    "np.random.seed(1671)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros da rede e do treinamento\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # número de outputs = número de dígitos\n",
    "#OPTIMIZER = RMSprop() # otimizador\n",
    "OPTIMIZER = Adam() # otimizador\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # quanto é reservado para validação\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando datasets de treino e teste\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train possui 60000 linhas de valores 28x28 --> reshape para 60000 x 784\n",
    "RESHAPED = 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 exemplos de treinamento\n",
      "10000 exemplos de teste\n"
     ]
    }
   ],
   "source": [
    "# Normalizando os dados\n",
    "# Tipicamente, os valores associados a cada pixel são normalizados na faixa [0, 1] \n",
    "# (o que significa que a intensidade de cada pixel é dividida por 255, o valor de intensidade máxima). \n",
    "# A saída é 10 classes, uma para cada dígito.\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'exemplos de treinamento')\n",
    "print(X_test.shape[0], 'exemplos de teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os vetores da class para matrizes binárias das classes\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria as camadas \n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape = (RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sumário da rede\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compila o modelo\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.5141 - accuracy: 0.8422 - val_loss: 0.1823 - val_accuracy: 0.9476\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2316 - accuracy: 0.9315 - val_loss: 0.1348 - val_accuracy: 0.9607\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.1739 - accuracy: 0.9478 - val_loss: 0.1098 - val_accuracy: 0.9667\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.1472 - accuracy: 0.9562 - val_loss: 0.1018 - val_accuracy: 0.9692\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1308 - accuracy: 0.9607 - val_loss: 0.0976 - val_accuracy: 0.9705\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.1177 - accuracy: 0.9643 - val_loss: 0.0878 - val_accuracy: 0.9734\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.1039 - accuracy: 0.9675 - val_loss: 0.0864 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.0968 - accuracy: 0.9691 - val_loss: 0.0873 - val_accuracy: 0.9747\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0889 - accuracy: 0.9724 - val_loss: 0.0849 - val_accuracy: 0.9750\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.0824 - accuracy: 0.9742 - val_loss: 0.0833 - val_accuracy: 0.9757\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.0780 - accuracy: 0.9759 - val_loss: 0.0819 - val_accuracy: 0.9769\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0711 - accuracy: 0.9772 - val_loss: 0.0837 - val_accuracy: 0.9767\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0703 - accuracy: 0.9774 - val_loss: 0.0844 - val_accuracy: 0.9765\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0665 - accuracy: 0.9783 - val_loss: 0.0874 - val_accuracy: 0.9766\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.0677 - accuracy: 0.9777 - val_loss: 0.0865 - val_accuracy: 0.9763\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.0613 - accuracy: 0.9799 - val_loss: 0.0845 - val_accuracy: 0.9765\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.0579 - accuracy: 0.9815 - val_loss: 0.0851 - val_accuracy: 0.9768\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0560 - accuracy: 0.9811 - val_loss: 0.0891 - val_accuracy: 0.9768\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.0538 - accuracy: 0.9827 - val_loss: 0.0913 - val_accuracy: 0.9764\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.0514 - accuracy: 0.9831 - val_loss: 0.0918 - val_accuracy: 0.9770\n"
     ]
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "modelo_v4 = model.fit(X_train, Y_train,\n",
    "                      batch_size = BATCH_SIZE, \n",
    "                      epochs = NB_EPOCH,\n",
    "                      verbose = VERBOSE, \n",
    "                      validation_split = VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 13us/step\n",
      "\n",
      "Test score: 0.0773961384162154\n",
      "Test accuracy: 0.9805999994277954\n"
     ]
    }
   ],
   "source": [
    "# Testa o modelo e imprime o score\n",
    "score = model.evaluate(X_test, Y_test, verbose = VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imprime os dados no modelo\n",
    "print(modelo_v4.history.keys())\n",
    "\n",
    "# Sumariza o modelo para acurácia\n",
    "plt.plot(modelo_v4.history['accuracy'])\n",
    "plt.plot(modelo_v4.history['val_accuracy'])\n",
    "plt.title('Acurácia do Modelo')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imprime a evolução de erro do modelo\n",
    "plt.plot(modelo_v4.history['loss'])\n",
    "plt.plot(modelo_v4.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Treino', 'Teste'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
